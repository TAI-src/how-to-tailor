{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to How to tailor \u00b6 Research in optimization tends to focus on how to improve the design or configuration of their methods from a theoretical perspective or motivated by empirical studies of algorithm behavior on either specific individual problems or broader benchmarking problem suites. Our Dagstuhl Seminar takes a different perspective \u2013 we wish to improve the algorithm practitioner, arming them with a recipe book for improving any one of a broad class of optimization methods (black-box methods) on any given problem, particularly focusing on real-world problems. In order to establish such a recipe book, we must look to real-world problems and first understand how they arrive on the desk of an optimization practitioner. The formulation of the problem itself is often (though not always) \u201cmessy\u201d with many crucial details missing (such as what the objective(s) are, what the decision space is, and what the constraints are)! But even when such details are apparently in place, the practitioner must remember they are choices, and it will be well to understand who has made those choices and why. At a second level, one must understand how many times the optimization problem must be solved, with what frequency, on what machines, with what data and simulators, and so on. At another level, the practitioner must understand uncertainty in the problem, how the solution is represented, how it will be used in practice, what current solution methods are used, and how they were arrived at. By considering these and a large host of other factors, the practitioner can then understand how they might go about leveraging all that domain knowledge to tailor an optimization method for that problem to achieve the best possible tuned method. The challenge for us all is to be able to do this for any given problem and to have tailoring strategies that work well for any (or a wide range of) black-box methods. We know that such general tailoring methods exist in broad outline in the advice given in old textbooks [1], but it is a long time since they were updated for modern optimization methods and modern infrastructure, such as our modern methods of benchmarking algorithms. The purpose of this Dagstuhl Seminar is to audit the current experience that exists in the field, pool it, and start to describe that experience in a way that goes beyond anecdote (although anecdotes can be useful) and moves us toward advice or recipes that actually work in practice. To achieve this, we aim to bring together researchers from a diverse range of backgrounds involved in real-world optimization from the customer side, the algorithm design or theoretical side, or practitioners who have already experienced successes and failures in the endeavor of tailoring algorithms to problems. The format of our seminar will be structured so that we hear from the cross-section of experienced and inexperienced practitioners, algorithm designers and customers, right from the start. We will then self-organize to achieve our aim of sharing our pooled experience and shaping it into practical, searchable and up-to-date advice on leveraging domain knowledge. [1] L. Davis, Handbook of genetic algorithms. Chapman & Hall, London, 1991. Example Collection \u00b6 As a first step, we are aiming to collect examples. Please add your examples (see below).","title":"Welcome to How to tailor"},{"location":"#welcome-to-how-to-tailor","text":"Research in optimization tends to focus on how to improve the design or configuration of their methods from a theoretical perspective or motivated by empirical studies of algorithm behavior on either specific individual problems or broader benchmarking problem suites. Our Dagstuhl Seminar takes a different perspective \u2013 we wish to improve the algorithm practitioner, arming them with a recipe book for improving any one of a broad class of optimization methods (black-box methods) on any given problem, particularly focusing on real-world problems. In order to establish such a recipe book, we must look to real-world problems and first understand how they arrive on the desk of an optimization practitioner. The formulation of the problem itself is often (though not always) \u201cmessy\u201d with many crucial details missing (such as what the objective(s) are, what the decision space is, and what the constraints are)! But even when such details are apparently in place, the practitioner must remember they are choices, and it will be well to understand who has made those choices and why. At a second level, one must understand how many times the optimization problem must be solved, with what frequency, on what machines, with what data and simulators, and so on. At another level, the practitioner must understand uncertainty in the problem, how the solution is represented, how it will be used in practice, what current solution methods are used, and how they were arrived at. By considering these and a large host of other factors, the practitioner can then understand how they might go about leveraging all that domain knowledge to tailor an optimization method for that problem to achieve the best possible tuned method. The challenge for us all is to be able to do this for any given problem and to have tailoring strategies that work well for any (or a wide range of) black-box methods. We know that such general tailoring methods exist in broad outline in the advice given in old textbooks [1], but it is a long time since they were updated for modern optimization methods and modern infrastructure, such as our modern methods of benchmarking algorithms. The purpose of this Dagstuhl Seminar is to audit the current experience that exists in the field, pool it, and start to describe that experience in a way that goes beyond anecdote (although anecdotes can be useful) and moves us toward advice or recipes that actually work in practice. To achieve this, we aim to bring together researchers from a diverse range of backgrounds involved in real-world optimization from the customer side, the algorithm design or theoretical side, or practitioners who have already experienced successes and failures in the endeavor of tailoring algorithms to problems. The format of our seminar will be structured so that we hear from the cross-section of experienced and inexperienced practitioners, algorithm designers and customers, right from the start. We will then self-organize to achieve our aim of sharing our pooled experience and shaping it into practical, searchable and up-to-date advice on leveraging domain knowledge. [1] L. Davis, Handbook of genetic algorithms. Chapman & Hall, London, 1991.","title":"Welcome to How to tailor"},{"location":"#example-collection","text":"As a first step, we are aiming to collect examples. Please add your examples (see below).","title":"Example Collection"},{"location":"examples/","text":"Example Index \u00b6 Optimisation for a Fleet of Healthcare Vehicles (Sarah Thomson) Aircraft design (Hauke Maathuis) Optimization of a GC-MS (mass spectrometer) for high-throughput analysis of biomedical samples - Development of ParEGO (Joshua Knowles) Identifying several diverse solutions rather than a single good one (Carola Doerr) Brachytherapy treatment planning (Anton Bouter) Engine design optimization (Robin Purshouse) Electric Motor Design Optimization (Tea Tu\u0161ar) Energy System Optimization (Mathijs de Weerdt) Automatic Train Operation (Carolin Mensendiek) Closed-loop optimization methods (Joshua Knowles, various co-authors at the time, Richard Allmendinger) Bayesian Optimization with Gaussian Processes for Protein Design (Carolin Benjamins) Building spatial design (Koen van der Blom) Combined Topology and Fibre-Orientation via Moving Morphable Components and Lamination Parameters (Iv\u00e1n Olarte Rodr\u00edguez, Gokhan Serhat, Mariusz Bujny, Thomas Baeck, Elena Raponi) Antenna control (Laurens Bliek) Deep RL in Ludii (general game playing) (Dennis Soemers) MarioGAN (Vanessa Volz) Multi-objective optimisation of a Stator (Tinkle Chugh) Optimizing Container turnover rate for a container yard by tuning parameters of a blackbox decision support system (Inneke Van Nieuwenhuyse) HPO for a speaker diarization model for emergency medical systems (Anja Jankovic)","title":"Example Index"},{"location":"examples/#example-index","text":"Optimisation for a Fleet of Healthcare Vehicles (Sarah Thomson) Aircraft design (Hauke Maathuis) Optimization of a GC-MS (mass spectrometer) for high-throughput analysis of biomedical samples - Development of ParEGO (Joshua Knowles) Identifying several diverse solutions rather than a single good one (Carola Doerr) Brachytherapy treatment planning (Anton Bouter) Engine design optimization (Robin Purshouse) Electric Motor Design Optimization (Tea Tu\u0161ar) Energy System Optimization (Mathijs de Weerdt) Automatic Train Operation (Carolin Mensendiek) Closed-loop optimization methods (Joshua Knowles, various co-authors at the time, Richard Allmendinger) Bayesian Optimization with Gaussian Processes for Protein Design (Carolin Benjamins) Building spatial design (Koen van der Blom) Combined Topology and Fibre-Orientation via Moving Morphable Components and Lamination Parameters (Iv\u00e1n Olarte Rodr\u00edguez, Gokhan Serhat, Mariusz Bujny, Thomas Baeck, Elena Raponi) Antenna control (Laurens Bliek) Deep RL in Ludii (general game playing) (Dennis Soemers) MarioGAN (Vanessa Volz) Multi-objective optimisation of a Stator (Tinkle Chugh) Optimizing Container turnover rate for a container yard by tuning parameters of a blackbox decision support system (Inneke Van Nieuwenhuyse) HPO for a speaker diarization model for emergency medical systems (Anja Jankovic)","title":"Example Index"},{"location":"examples/example_1/","text":"Optimisation for a Fleet of Healthcare Vehicles \u00b6 Problem Description \u00b6 A healthcare provider in a region of Scotland (Argyll and Bute) wanted to reduce their vehicle fleet size while still being able to cater for all trips. They provided 4 months of historical data about where their existing fleet were based and the trips they conducted, including start and end times and geographic location. We were also given information about the vehicle types and which vehicles were allowed to do which trips. Why was tailoring needed? \u00b6 Not too much tailoring was needed but there were some particulars that had to be accounted for: Jobs (i.e. trips) have a type of vehicle which (historically) executed them, but if needed certain other types of vehicles can do the trip. For example, a small car originally did the trip, can be done by a van. Vehicles can be swapped between geographical bases if needed and if the swap does not mean that the vehicle home base cannot cover its own trips. It does not make sense to try and remove a type of vehicle from a base if there are none there or maybe if there are a small amount there. This led to a semi-guided mutation design. Baseline algorithm \u00b6 Upper level: stochastic local search; lower level: constructive heuristic. Motivations for choice: we wanted to keep it simple as possible and explainable for the user. No need to use fancy algorithms if a simple approach can obtain results. Tailoring process \u00b6 Adding in constraints (part of the operators); added additional vehicle/machine swap operation; semi-guided mutation. What was tailored \u00b6 Aspects of the algorithmic operators were tailored. This included the nature of the mutation operator and how it ensured that mutated solutions are feasible within the specific constraints of the problem. Main problem characteristics \u00b6 Choose most important ones: low-dimensional at upper level, high-dimensional at lower level; highly constrained (some soft and some hard); offline; there is an existing solution that works(current fleet); is a simplified version of what is eventually sought (optimising routes, carbon as well); low data sensitivity. References \u00b6 No response Author \u00b6 Sarah Thomson","title":"Optimisation for a Fleet of Healthcare Vehicles"},{"location":"examples/example_1/#optimisation-for-a-fleet-of-healthcare-vehicles","text":"","title":"Optimisation for a Fleet of Healthcare Vehicles"},{"location":"examples/example_1/#problem-description","text":"A healthcare provider in a region of Scotland (Argyll and Bute) wanted to reduce their vehicle fleet size while still being able to cater for all trips. They provided 4 months of historical data about where their existing fleet were based and the trips they conducted, including start and end times and geographic location. We were also given information about the vehicle types and which vehicles were allowed to do which trips.","title":"Problem Description"},{"location":"examples/example_1/#why-was-tailoring-needed","text":"Not too much tailoring was needed but there were some particulars that had to be accounted for: Jobs (i.e. trips) have a type of vehicle which (historically) executed them, but if needed certain other types of vehicles can do the trip. For example, a small car originally did the trip, can be done by a van. Vehicles can be swapped between geographical bases if needed and if the swap does not mean that the vehicle home base cannot cover its own trips. It does not make sense to try and remove a type of vehicle from a base if there are none there or maybe if there are a small amount there. This led to a semi-guided mutation design.","title":"Why was tailoring needed?"},{"location":"examples/example_1/#baseline-algorithm","text":"Upper level: stochastic local search; lower level: constructive heuristic. Motivations for choice: we wanted to keep it simple as possible and explainable for the user. No need to use fancy algorithms if a simple approach can obtain results.","title":"Baseline algorithm"},{"location":"examples/example_1/#tailoring-process","text":"Adding in constraints (part of the operators); added additional vehicle/machine swap operation; semi-guided mutation.","title":"Tailoring process"},{"location":"examples/example_1/#what-was-tailored","text":"Aspects of the algorithmic operators were tailored. This included the nature of the mutation operator and how it ensured that mutated solutions are feasible within the specific constraints of the problem.","title":"What was tailored"},{"location":"examples/example_1/#main-problem-characteristics","text":"Choose most important ones: low-dimensional at upper level, high-dimensional at lower level; highly constrained (some soft and some hard); offline; there is an existing solution that works(current fleet); is a simplified version of what is eventually sought (optimising routes, carbon as well); low data sensitivity.","title":"Main problem characteristics"},{"location":"examples/example_1/#references","text":"No response","title":"References"},{"location":"examples/example_1/#author","text":"Sarah Thomson","title":"Author"},{"location":"examples/example_10/","text":"Aircraft design \u00b6 Problem Description \u00b6 The problem was to solve an optimisation problem to lower the weight of an aircraft wing. During this optimisation many constraints arising from multiple disciplines need to be taken into account to ensure feasibility of the design (e.g. structural stability, manufacturability, \u2026). Why was tailoring needed? \u00b6 To reach the best design possible, one ideally wants to define as many design variables as possible to increase the design freedom of the system. That very quickly poses a high-dimensional design problem. Additionally, in BO black-box constraints are usually taken into account via separate surrogate models. In cases where the number of constraints is high (>10^3) this quickly becomes a bottleneck in terms of computational resources. Baseline algorithm \u00b6 Since the constraints computed via the respective disciplines can be either highly nonlinear, discontinuous or being computed via commercial solvers that do not provide gradients BO was chosen as a starting point. Tailoring process \u00b6 Developed were two approaches that leverage dimensionality reduction in the input and/or output space, respectively, reducing the number of needed GPs. The acquisition function uses a trust region heuristic to mitigate the curse of dimensionality. What was tailored \u00b6 No response Main problem characteristics \u00b6 Cont. variables single objective \u201cHigh-dimensionality\u201d Budget is limited parallelisation possible References \u00b6 No response Contact information (optional) \u00b6 No response Author \u00b6 Hauke Maathuis No response","title":"Aircraft design"},{"location":"examples/example_10/#aircraft-design","text":"","title":"Aircraft design"},{"location":"examples/example_10/#problem-description","text":"The problem was to solve an optimisation problem to lower the weight of an aircraft wing. During this optimisation many constraints arising from multiple disciplines need to be taken into account to ensure feasibility of the design (e.g. structural stability, manufacturability, \u2026).","title":"Problem Description"},{"location":"examples/example_10/#why-was-tailoring-needed","text":"To reach the best design possible, one ideally wants to define as many design variables as possible to increase the design freedom of the system. That very quickly poses a high-dimensional design problem. Additionally, in BO black-box constraints are usually taken into account via separate surrogate models. In cases where the number of constraints is high (>10^3) this quickly becomes a bottleneck in terms of computational resources.","title":"Why was tailoring needed?"},{"location":"examples/example_10/#baseline-algorithm","text":"Since the constraints computed via the respective disciplines can be either highly nonlinear, discontinuous or being computed via commercial solvers that do not provide gradients BO was chosen as a starting point.","title":"Baseline algorithm"},{"location":"examples/example_10/#tailoring-process","text":"Developed were two approaches that leverage dimensionality reduction in the input and/or output space, respectively, reducing the number of needed GPs. The acquisition function uses a trust region heuristic to mitigate the curse of dimensionality.","title":"Tailoring process"},{"location":"examples/example_10/#what-was-tailored","text":"No response","title":"What was tailored"},{"location":"examples/example_10/#main-problem-characteristics","text":"Cont. variables single objective \u201cHigh-dimensionality\u201d Budget is limited parallelisation possible","title":"Main problem characteristics"},{"location":"examples/example_10/#references","text":"No response","title":"References"},{"location":"examples/example_10/#contact-information-optional","text":"No response","title":"Contact information (optional)"},{"location":"examples/example_10/#author","text":"Hauke Maathuis No response","title":"Author"},{"location":"examples/example_11/","text":"Optimization of a GC-MS (mass spectrometer) for high-throughput analysis of biomedical samples - Development of ParEGO \u00b6 Problem Description \u00b6 Configure the parameters of an analytical instrument (a GC-MS machine) such that it would obtain a high-resolution identification of all the constituents of a complex sample (such as human blood with all the metabolic products in it identified). No simulator of the GC-MS was available or would be realistic. Only one GC-MS machine was available (i.e., parallelization is not possible). Evaluation of one configuration on one sample requires of the order of 15 minutes to 75 minutes, depending on settings. Minimizing the processing time of the GC-MS machine to produce its output was an additional important objective. Why was tailoring needed? \u00b6 Baselines considered were multiobjective evolutionary algorithms. The motivations were three-fold * No analytical function: black-box setting * Multiple objectives, at least 3: number of peaks, signal-to-noise, processing time * Familiarity with multiobjective EAs Tailoring was needed because these methods require order 1000\u2019s of evaluations and are not designed for optimization under a much more limited budget. Our budget was set at 120 experiments, initially. Baseline algorithm \u00b6 No response Tailoring process \u00b6 We first tailored PAES, PESA, and NSGA-II as leading EMO algorithms of the time Mostly this is about reducing the population size significantly, and trying to find reasonable mutation rates. All of this was done in simulation of course, because you cannot tune on the real problem! We then investigated single-objective methods for expensive problems. We chose to adapt EGO (Jones et al), which by today\u2019s classification is a Bayesian optimization method, but at that time was known as a DACE-model- or Kriging-metamodel-based method. We then tailored (really, generalized) EGO for the multiobjective case by introducing a randomized sampling of scalarization functions, and Adding archiving methods, and Experimenting with searching over the Kriging metamodel (\u201cacquistion function\u201d) with alternatives to branch-and-bound or downhill simplex (used in the original EGO paper); eventually we used an inner EA with niching to ensure good search of this multimodal landscape. What was tailored \u00b6 No response Main problem characteristics \u00b6 Problem properties Blackbox - a real physical experiment No gradients No models (initially) Noisy List of feasible algorithms: PAES, PESA, NSGA-II, or adapt EGO to the multiobjective case, i.e., develop ParEGO Existing alternatives / solutions: none Search space Continuous / mixed Low - up to 15 Objective Space 3 objectives - finding good tradeoffs very important 15 minutes-75 minutes for one evaluation (of all objectives) of one solution Noise in fitness evaluation The definition of the objective(s) were changing during the first few evaluations as we honed our ability to measure signal-noise ratio and to count peaks in the output chromatograms. Constraints Constraint-types: mixed. Some constraints were known a priori; some we learnt as we progressed, i.e., the machine failed for some settings User needs Biggest factor was time on the machine. We needed to optimize the settings for a much larger study so that thousands of samples could be analysed using the \u201coptimal\u201d settings. We had weeks to achieve this. So all tuning and algorithm design decisions have to be made offline, e.g. using either intuition or tuning on other functions. A team with very different expertise was needed to undertake this whole process of developing an optimized GC-MS setting. Communication was very challenging. A notable remark from the senior scientist (to me) was, \u201cJosh, your algorithm appears to be doing entirely random experiments\u201d. (I suggest, now that ParEGO is quite well established, he was wrong. But of course it looked like that from an onlooker perspective). Notice that to tune and compare algorithms, we simplified functions in the multiobjective EA test suites of the day. The simplification is motivated and described properly in the ParEGO paper cited above. References \u00b6 Closed-Loop, Multiobjective Optimization of Analytical Instrumentation:\u2009 Gas Chromatography/Time-of-Flight Mass Spectrometry of the Metabolomes of Human Serum and of Yeast Fermentations Steve O'Hagan, Warwick B. Dunn, Marie Brown, Joshua D. Knowles, and Douglas B. Kell. Analytical Chemistry 2005 77 (1), 290-303. DOI: 10.1021/ac049146x ParEGO: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems Joshua Knowles. IEEE Transactions on Evolutionary Computation 2006 10(1), 50-66. doi: 10.1109/TEVC.2005.851274 Contact information (optional) \u00b6 No response Author \u00b6 Joshua Knowles No response","title":"Optimization of a GC-MS (mass spectrometer) for high-throughput analysis of biomedical samples - Development of ParEGO"},{"location":"examples/example_11/#optimization-of-a-gc-ms-mass-spectrometer-for-high-throughput-analysis-of-biomedical-samples-development-of-parego","text":"","title":"Optimization of a GC-MS (mass spectrometer) for high-throughput analysis of biomedical samples - Development of ParEGO"},{"location":"examples/example_11/#problem-description","text":"Configure the parameters of an analytical instrument (a GC-MS machine) such that it would obtain a high-resolution identification of all the constituents of a complex sample (such as human blood with all the metabolic products in it identified). No simulator of the GC-MS was available or would be realistic. Only one GC-MS machine was available (i.e., parallelization is not possible). Evaluation of one configuration on one sample requires of the order of 15 minutes to 75 minutes, depending on settings. Minimizing the processing time of the GC-MS machine to produce its output was an additional important objective.","title":"Problem Description"},{"location":"examples/example_11/#why-was-tailoring-needed","text":"Baselines considered were multiobjective evolutionary algorithms. The motivations were three-fold * No analytical function: black-box setting * Multiple objectives, at least 3: number of peaks, signal-to-noise, processing time * Familiarity with multiobjective EAs Tailoring was needed because these methods require order 1000\u2019s of evaluations and are not designed for optimization under a much more limited budget. Our budget was set at 120 experiments, initially.","title":"Why was tailoring needed?"},{"location":"examples/example_11/#baseline-algorithm","text":"No response","title":"Baseline algorithm"},{"location":"examples/example_11/#tailoring-process","text":"We first tailored PAES, PESA, and NSGA-II as leading EMO algorithms of the time Mostly this is about reducing the population size significantly, and trying to find reasonable mutation rates. All of this was done in simulation of course, because you cannot tune on the real problem! We then investigated single-objective methods for expensive problems. We chose to adapt EGO (Jones et al), which by today\u2019s classification is a Bayesian optimization method, but at that time was known as a DACE-model- or Kriging-metamodel-based method. We then tailored (really, generalized) EGO for the multiobjective case by introducing a randomized sampling of scalarization functions, and Adding archiving methods, and Experimenting with searching over the Kriging metamodel (\u201cacquistion function\u201d) with alternatives to branch-and-bound or downhill simplex (used in the original EGO paper); eventually we used an inner EA with niching to ensure good search of this multimodal landscape.","title":"Tailoring process"},{"location":"examples/example_11/#what-was-tailored","text":"No response","title":"What was tailored"},{"location":"examples/example_11/#main-problem-characteristics","text":"Problem properties Blackbox - a real physical experiment No gradients No models (initially) Noisy List of feasible algorithms: PAES, PESA, NSGA-II, or adapt EGO to the multiobjective case, i.e., develop ParEGO Existing alternatives / solutions: none Search space Continuous / mixed Low - up to 15 Objective Space 3 objectives - finding good tradeoffs very important 15 minutes-75 minutes for one evaluation (of all objectives) of one solution Noise in fitness evaluation The definition of the objective(s) were changing during the first few evaluations as we honed our ability to measure signal-noise ratio and to count peaks in the output chromatograms. Constraints Constraint-types: mixed. Some constraints were known a priori; some we learnt as we progressed, i.e., the machine failed for some settings User needs Biggest factor was time on the machine. We needed to optimize the settings for a much larger study so that thousands of samples could be analysed using the \u201coptimal\u201d settings. We had weeks to achieve this. So all tuning and algorithm design decisions have to be made offline, e.g. using either intuition or tuning on other functions. A team with very different expertise was needed to undertake this whole process of developing an optimized GC-MS setting. Communication was very challenging. A notable remark from the senior scientist (to me) was, \u201cJosh, your algorithm appears to be doing entirely random experiments\u201d. (I suggest, now that ParEGO is quite well established, he was wrong. But of course it looked like that from an onlooker perspective). Notice that to tune and compare algorithms, we simplified functions in the multiobjective EA test suites of the day. The simplification is motivated and described properly in the ParEGO paper cited above.","title":"Main problem characteristics"},{"location":"examples/example_11/#references","text":"Closed-Loop, Multiobjective Optimization of Analytical Instrumentation:\u2009 Gas Chromatography/Time-of-Flight Mass Spectrometry of the Metabolomes of Human Serum and of Yeast Fermentations Steve O'Hagan, Warwick B. Dunn, Marie Brown, Joshua D. Knowles, and Douglas B. Kell. Analytical Chemistry 2005 77 (1), 290-303. DOI: 10.1021/ac049146x ParEGO: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems Joshua Knowles. IEEE Transactions on Evolutionary Computation 2006 10(1), 50-66. doi: 10.1109/TEVC.2005.851274","title":"References"},{"location":"examples/example_11/#contact-information-optional","text":"No response","title":"Contact information (optional)"},{"location":"examples/example_11/#author","text":"Joshua Knowles No response","title":"Author"},{"location":"examples/example_12/","text":"Identifying several diverse solutions rather than a single good one \u00b6 Problem Description \u00b6 No response Why was tailoring needed? \u00b6 Rather than outputting a single good solution, users (for example in engineering) may prefer to see a set of diverse solutions that are all reasonable alternatives. Common approaches such as multimodal optimizers or quality-diversity algorithms don't have a guarantee on the diversity in the configuration space, which we request to be a minimal distance between any two solutions in the batch that is output. We also investigated the quality of the batches that can be obtained from the search trajectories of high-performing solvers such as cma-es. Since none of these approaches gave satisfying results, we developed a parallel CMA-ES variant that uses cascading taboo regions that encode the diversity criterion. Baseline algorithm \u00b6 CMA-ES, because 1. it is known to perform well when searching for a single good solution, 2. it is easy to adapt (well-maintained and documented code), 3. we are familiar enough with the algorithm to adjust it (and to trust its performance) Tailoring process \u00b6 We tried several other ideas such as repelling swarm algorithms but after some initial tests decided to go with this algorithm and to tailor it. Some specific details had to be figured out, we used standard benchmarking routines in the development process. What was tailored (Problem model, mutation operator) Algorithm (new: parallelization and cascading taboo regions) What was tailored \u00b6 No response Main problem characteristics \u00b6 Choose most important ones Interest in having batches of diverse solutions came from infeasibility constraints that are not encoded in the objective function (e.g., manufacturing constraints) References \u00b6 https://arxiv.org/abs/2502.13730 for an algorithm https://arxiv.org/abs/2408.16393 for why tailoring was needed Contact information (optional) \u00b6 No response Author \u00b6 Carola Doerr No response","title":"Identifying several diverse solutions rather than a single good one"},{"location":"examples/example_12/#identifying-several-diverse-solutions-rather-than-a-single-good-one","text":"","title":"Identifying several diverse solutions rather than a single good one"},{"location":"examples/example_12/#problem-description","text":"No response","title":"Problem Description"},{"location":"examples/example_12/#why-was-tailoring-needed","text":"Rather than outputting a single good solution, users (for example in engineering) may prefer to see a set of diverse solutions that are all reasonable alternatives. Common approaches such as multimodal optimizers or quality-diversity algorithms don't have a guarantee on the diversity in the configuration space, which we request to be a minimal distance between any two solutions in the batch that is output. We also investigated the quality of the batches that can be obtained from the search trajectories of high-performing solvers such as cma-es. Since none of these approaches gave satisfying results, we developed a parallel CMA-ES variant that uses cascading taboo regions that encode the diversity criterion.","title":"Why was tailoring needed?"},{"location":"examples/example_12/#baseline-algorithm","text":"CMA-ES, because 1. it is known to perform well when searching for a single good solution, 2. it is easy to adapt (well-maintained and documented code), 3. we are familiar enough with the algorithm to adjust it (and to trust its performance)","title":"Baseline algorithm"},{"location":"examples/example_12/#tailoring-process","text":"We tried several other ideas such as repelling swarm algorithms but after some initial tests decided to go with this algorithm and to tailor it. Some specific details had to be figured out, we used standard benchmarking routines in the development process. What was tailored (Problem model, mutation operator) Algorithm (new: parallelization and cascading taboo regions)","title":"Tailoring process"},{"location":"examples/example_12/#what-was-tailored","text":"No response","title":"What was tailored"},{"location":"examples/example_12/#main-problem-characteristics","text":"Choose most important ones Interest in having batches of diverse solutions came from infeasibility constraints that are not encoded in the objective function (e.g., manufacturing constraints)","title":"Main problem characteristics"},{"location":"examples/example_12/#references","text":"https://arxiv.org/abs/2502.13730 for an algorithm https://arxiv.org/abs/2408.16393 for why tailoring was needed","title":"References"},{"location":"examples/example_12/#contact-information-optional","text":"No response","title":"Contact information (optional)"},{"location":"examples/example_12/#author","text":"Carola Doerr No response","title":"Author"},{"location":"examples/example_13/","text":"Brachytherapy treatment planning \u00b6 Problem Description \u00b6 Brachytherapy is a form of internal radiation therapy, which is used to treat patients with, e.g., prostate, cervical, or breast cancer. Radiation is delivered by a radioactive source that is guided through catheters inserted into the target volume. Optimization parameters are so-called dwell times, which determine for how long the radioactive source is halted at each of the predetermined dwell positions, delivering dose to the tissue surrounding the dwell position. Objectives concern delivering a sufficient amount of dose to the target volume, and sparing the surrounding healthy organs, based on values specified by a clinical protocol. Why was tailoring needed? \u00b6 Conventional methods optimize a simplified objective function that does not correlate well with values in clinical protocol that are evaluated by clinicians. Relative weights between clinical aims are difficult to establish and depend on patient characteristics that are difficult to establish. Baseline algorithm \u00b6 Multi-objective real-valued gene-pool optimal mixing evolutionary algorithm (MO-RV-GOMEA). Was shown to perform best among multi-objective EAs. Key strength is the gene-pool optimal mixing (GOM) variation operator, offering high selection pressure and structure-informed variation. Tailoring process \u00b6 Problem formulation : Aggregation of clinical aims into 2/3 objectives that are optimized in a worst-case manner with respect to aim in clinical protocol. Due to worst-case formulation, objective value indicates whether all clinical aims are satisfied. Many-objective optimization with 10-20 objectives was attempted, but did not have satisfactory results in the most interesting region of the search space. Additional constraints/objectives were added in an iterative process, e.g., to prevent dose hotspots or excessive dose in specific regions. Satisfaction of all aims in the clinical protocol was not sufficient to obtain high-quality treatment plans. Parallelisation of evaluation function : Clinical workflow requires optimization within at most a few minutes, which initial implementations did not satisfy. This parallelisation requires full knowledge of the dose calculation model used during the evaluation. Dependency structure: Knowledge of the optimization problem is used to specify a dependency structure for the GOM variation operator, based on Euclidean distance of dwell positions in 3D space. Partial evaluations: More efficient evaluation after partial variation, as is done in GOM. Requires problem knowledge. Tuning of population size/hyperparameters What was tailored \u00b6 No response Main problem characteristics \u00b6 Real-valued 100-500 dimensional depending on patient Multi/many-objective Non-convex Constrained (generally easy to satisfy) Adjustable accuracy of objective function for higher computational cost Parallelisation beyond population-level; resulting in relatively inexpensive evaluation <5min runtime budget References \u00b6 No response Contact information (optional) \u00b6 No response Author \u00b6 Anton Bouter No response","title":"Brachytherapy treatment planning"},{"location":"examples/example_13/#brachytherapy-treatment-planning","text":"","title":"Brachytherapy treatment planning"},{"location":"examples/example_13/#problem-description","text":"Brachytherapy is a form of internal radiation therapy, which is used to treat patients with, e.g., prostate, cervical, or breast cancer. Radiation is delivered by a radioactive source that is guided through catheters inserted into the target volume. Optimization parameters are so-called dwell times, which determine for how long the radioactive source is halted at each of the predetermined dwell positions, delivering dose to the tissue surrounding the dwell position. Objectives concern delivering a sufficient amount of dose to the target volume, and sparing the surrounding healthy organs, based on values specified by a clinical protocol.","title":"Problem Description"},{"location":"examples/example_13/#why-was-tailoring-needed","text":"Conventional methods optimize a simplified objective function that does not correlate well with values in clinical protocol that are evaluated by clinicians. Relative weights between clinical aims are difficult to establish and depend on patient characteristics that are difficult to establish.","title":"Why was tailoring needed?"},{"location":"examples/example_13/#baseline-algorithm","text":"Multi-objective real-valued gene-pool optimal mixing evolutionary algorithm (MO-RV-GOMEA). Was shown to perform best among multi-objective EAs. Key strength is the gene-pool optimal mixing (GOM) variation operator, offering high selection pressure and structure-informed variation.","title":"Baseline algorithm"},{"location":"examples/example_13/#tailoring-process","text":"Problem formulation : Aggregation of clinical aims into 2/3 objectives that are optimized in a worst-case manner with respect to aim in clinical protocol. Due to worst-case formulation, objective value indicates whether all clinical aims are satisfied. Many-objective optimization with 10-20 objectives was attempted, but did not have satisfactory results in the most interesting region of the search space. Additional constraints/objectives were added in an iterative process, e.g., to prevent dose hotspots or excessive dose in specific regions. Satisfaction of all aims in the clinical protocol was not sufficient to obtain high-quality treatment plans. Parallelisation of evaluation function : Clinical workflow requires optimization within at most a few minutes, which initial implementations did not satisfy. This parallelisation requires full knowledge of the dose calculation model used during the evaluation. Dependency structure: Knowledge of the optimization problem is used to specify a dependency structure for the GOM variation operator, based on Euclidean distance of dwell positions in 3D space. Partial evaluations: More efficient evaluation after partial variation, as is done in GOM. Requires problem knowledge. Tuning of population size/hyperparameters","title":"Tailoring process"},{"location":"examples/example_13/#what-was-tailored","text":"No response","title":"What was tailored"},{"location":"examples/example_13/#main-problem-characteristics","text":"Real-valued 100-500 dimensional depending on patient Multi/many-objective Non-convex Constrained (generally easy to satisfy) Adjustable accuracy of objective function for higher computational cost Parallelisation beyond population-level; resulting in relatively inexpensive evaluation <5min runtime budget","title":"Main problem characteristics"},{"location":"examples/example_13/#references","text":"No response","title":"References"},{"location":"examples/example_13/#contact-information-optional","text":"No response","title":"Contact information (optional)"},{"location":"examples/example_13/#author","text":"Anton Bouter No response","title":"Author"},{"location":"examples/example_14/","text":"Engine design optimization \u00b6 Problem Description \u00b6 Optimization of the hardware and control system for a new 1.0 litre 3-cylinder gasoline direct injection (GDI) turbocharged engine with a low-pressure exhaust gas recirculation (LP EGR) system for Ford Motor Company A multi-objective optimization problem seeking to minimize fuel consumption and NOx emissions over a two-minute dynamic duty cycle, subject to five constraints (turbine inlet temperature, number of knock occurrences, peak cylinder pressure, peak cylinder pressure rise, total work). Seven decision variables are defined: four define the hardware choices of cylinder compression ratio, turbo machinery and EGR cooler sizing; three relate to control variables that parameterise the engine control logic. Why was tailoring needed? \u00b6 Combination of problem features not yet addressed by an available algorithm: multi-objective, constrained, mixed-integer, expensive Baseline algorithm \u00b6 ParEGO (was already implemented in the Tigon library that underpins our Liger open-source MCDM workflow tool being developed in collaboration with Ford) Tailoring process \u00b6 The Tigon ParEGO implementation uses ACROMUSE as the internal optimizer (this is standard for Tigon but not the vanilla ParEGO) Integrated two constraint handling alternatives: penalty function and probability of feasibility (the penalty function approach turned out to be more effective) For the mixed integer representation, the surrogate assumed a continuous surface based on an integer coding, but ACROMUSE was fitted with existing discrete variation operators from the literature: Yu & Gen\u2019s discrete crossover and Rudolph\u2019s discrete mutation For the evaluation, we collaborated with Hartree to develop an HPC plug-in for the Liger workflow software Benchmarking (against existing Ford methods: DoE and NSGA-II) was done using the speed reducer and OSY problems (which have similar features to the real-world problem, although only the speed reducer problem has a (single) discrete decision variable) What was tailored \u00b6 No response Main problem characteristics \u00b6 Multi-objective (discovered post hoc to have a convex Pareto front - we used an infinity norm for the scalarising function but the problem would likely benefit from using the 1-norm) Constrained (discovered post hoc that four constraints are active) Mixed-integer (four discrete and three continuous) Expensive (several hours to evaluate a single design over HPC; nominal budget of 500 design alternatives) Nominal candidate design available (which turned out to be dominated) References \u00b6 https://doi.org/10.1016/j.ejor.2022.08.032; https://doi.org/10.1016/j.asoc.2020.106851; https://github.com/ligerdev/liger; https://www.apcuk.co.uk/impact/funded-projects/ford-motor-company-dynamo/ (note that the engine model is proprietary to Ford; the model was developed by collaborators at the University of Bath as part of the project and is implemented in Mathworks Matlab Simulink, including a physics-based co-simulation model developed in Ricardo WAVE-RT). Contact information (optional) \u00b6 No response Author \u00b6 Robin Purshouse No response","title":"Engine design optimization"},{"location":"examples/example_14/#engine-design-optimization","text":"","title":"Engine design optimization"},{"location":"examples/example_14/#problem-description","text":"Optimization of the hardware and control system for a new 1.0 litre 3-cylinder gasoline direct injection (GDI) turbocharged engine with a low-pressure exhaust gas recirculation (LP EGR) system for Ford Motor Company A multi-objective optimization problem seeking to minimize fuel consumption and NOx emissions over a two-minute dynamic duty cycle, subject to five constraints (turbine inlet temperature, number of knock occurrences, peak cylinder pressure, peak cylinder pressure rise, total work). Seven decision variables are defined: four define the hardware choices of cylinder compression ratio, turbo machinery and EGR cooler sizing; three relate to control variables that parameterise the engine control logic.","title":"Problem Description"},{"location":"examples/example_14/#why-was-tailoring-needed","text":"Combination of problem features not yet addressed by an available algorithm: multi-objective, constrained, mixed-integer, expensive","title":"Why was tailoring needed?"},{"location":"examples/example_14/#baseline-algorithm","text":"ParEGO (was already implemented in the Tigon library that underpins our Liger open-source MCDM workflow tool being developed in collaboration with Ford)","title":"Baseline algorithm"},{"location":"examples/example_14/#tailoring-process","text":"The Tigon ParEGO implementation uses ACROMUSE as the internal optimizer (this is standard for Tigon but not the vanilla ParEGO) Integrated two constraint handling alternatives: penalty function and probability of feasibility (the penalty function approach turned out to be more effective) For the mixed integer representation, the surrogate assumed a continuous surface based on an integer coding, but ACROMUSE was fitted with existing discrete variation operators from the literature: Yu & Gen\u2019s discrete crossover and Rudolph\u2019s discrete mutation For the evaluation, we collaborated with Hartree to develop an HPC plug-in for the Liger workflow software Benchmarking (against existing Ford methods: DoE and NSGA-II) was done using the speed reducer and OSY problems (which have similar features to the real-world problem, although only the speed reducer problem has a (single) discrete decision variable)","title":"Tailoring process"},{"location":"examples/example_14/#what-was-tailored","text":"No response","title":"What was tailored"},{"location":"examples/example_14/#main-problem-characteristics","text":"Multi-objective (discovered post hoc to have a convex Pareto front - we used an infinity norm for the scalarising function but the problem would likely benefit from using the 1-norm) Constrained (discovered post hoc that four constraints are active) Mixed-integer (four discrete and three continuous) Expensive (several hours to evaluate a single design over HPC; nominal budget of 500 design alternatives) Nominal candidate design available (which turned out to be dominated)","title":"Main problem characteristics"},{"location":"examples/example_14/#references","text":"https://doi.org/10.1016/j.ejor.2022.08.032; https://doi.org/10.1016/j.asoc.2020.106851; https://github.com/ligerdev/liger; https://www.apcuk.co.uk/impact/funded-projects/ford-motor-company-dynamo/ (note that the engine model is proprietary to Ford; the model was developed by collaborators at the University of Bath as part of the project and is implemented in Mathworks Matlab Simulink, including a physics-based co-simulation model developed in Ricardo WAVE-RT).","title":"References"},{"location":"examples/example_14/#contact-information-optional","text":"No response","title":"Contact information (optional)"},{"location":"examples/example_14/#author","text":"Robin Purshouse No response","title":"Author"},{"location":"examples/example_15/","text":"Electric Motor Design Optimization \u00b6 Problem Description \u00b6 The goal was to find a design of a synchronous electric motor for power steering systems that minimizes costs and satisfies all constraints. While computing the costs was rather straightforward, checking the feasibility of solutions required time-consuming simulations. Why was tailoring needed? \u00b6 Although the company was already using surrogate-based optimization before our collaboration, the severe constraints were making it hard to find high-quality solutions in reasonable time. The tailoring made it possible to find designs that were 10% cheaper of the ones used by the company. Baseline algorithm \u00b6 lq-CMA-ES [Hansen2019], which uses a surrogate model and also supports constraints Tailoring process \u00b6 Tailoring was performed on the evaluation of solutions rather than the optimization algorithm. The evaluation of solutions was split into five steps. If after any step, the solution was found to be infeasible, the evaluation stopped there and a penalty was added to the objective. The steps were: 1. Executing a quick Python script to check feasibility with regard to magnet placement. Checking this before evaluating a solution with the simulator was particularly helpful because we could avoid some simulation errors. About 17.5% of solutions were discarded after this step, which spared about 8 minutes of time per solution. 2. Executing a simulation with Ansys to compute the properties of the electrical motor design. This took about 1 minute when successful and considerably longer when the design was infeasible. About 29.4% of solutions were discarded after this step, which spared about 7 minutes of time per solution. 3. Computing the costs with a quick Python script. 4. Checking solution robustness. This consisted of running parallel simulations with Ansys that took about 7 minutes per solution. This step was tailored further (compared to previous experiments done by the company) by adapting the mesh size as some additional experiments showed that we could gain about half the time without losing too much information. 5. Computing some additional properties with a quick Python script and checking final solution feasibility. The objective function was tailored to accommodate some \u201csoft\u201d constraints as well as penalize infeasible solutions (the earlier a solution was found to be infeasible, the larger the penalty). Because of the limited time at our disposal, we didn\u2019t attempt to tune the optimization algorithm. What was tailored \u00b6 No response Main problem characteristics \u00b6 Problem definition * 13 variables * 12 \u201ccontinuous\u201d (were discretized in evaluations, but were treated as continuous by the optimization algorithm) * 1 discrete (ordered, not combinatorial) * 1 objective (costs) * 2 \u201csoft\u201d constraints, whose violation was added to the main objective * 10 \u201chard\u201d constraints Other properties * The constraints were multimodal and hard to satisfy * While we were working on a single problem instance, the company had many instances to solve that could be handled in the same way * We could only afford to run the algorithm a handful of times, for a few thousand evaluations each * The code cannot be disclosed References \u00b6 [Hansen2019] Nikolaus Hansen. 2019. A global surrogate assisted CMA-ES. V Proceedings of the 2019 Genetic and Evolutionary Computation Conference. ACM, New York, NY, USA, 664\u2013672. https://doi.org/10.1145/3321707.3321842 Contact information (optional) \u00b6 No response Author \u00b6 Tea Tu\u0161ar No response","title":"Electric Motor Design Optimization"},{"location":"examples/example_15/#electric-motor-design-optimization","text":"","title":"Electric Motor Design Optimization"},{"location":"examples/example_15/#problem-description","text":"The goal was to find a design of a synchronous electric motor for power steering systems that minimizes costs and satisfies all constraints. While computing the costs was rather straightforward, checking the feasibility of solutions required time-consuming simulations.","title":"Problem Description"},{"location":"examples/example_15/#why-was-tailoring-needed","text":"Although the company was already using surrogate-based optimization before our collaboration, the severe constraints were making it hard to find high-quality solutions in reasonable time. The tailoring made it possible to find designs that were 10% cheaper of the ones used by the company.","title":"Why was tailoring needed?"},{"location":"examples/example_15/#baseline-algorithm","text":"lq-CMA-ES [Hansen2019], which uses a surrogate model and also supports constraints","title":"Baseline algorithm"},{"location":"examples/example_15/#tailoring-process","text":"Tailoring was performed on the evaluation of solutions rather than the optimization algorithm. The evaluation of solutions was split into five steps. If after any step, the solution was found to be infeasible, the evaluation stopped there and a penalty was added to the objective. The steps were: 1. Executing a quick Python script to check feasibility with regard to magnet placement. Checking this before evaluating a solution with the simulator was particularly helpful because we could avoid some simulation errors. About 17.5% of solutions were discarded after this step, which spared about 8 minutes of time per solution. 2. Executing a simulation with Ansys to compute the properties of the electrical motor design. This took about 1 minute when successful and considerably longer when the design was infeasible. About 29.4% of solutions were discarded after this step, which spared about 7 minutes of time per solution. 3. Computing the costs with a quick Python script. 4. Checking solution robustness. This consisted of running parallel simulations with Ansys that took about 7 minutes per solution. This step was tailored further (compared to previous experiments done by the company) by adapting the mesh size as some additional experiments showed that we could gain about half the time without losing too much information. 5. Computing some additional properties with a quick Python script and checking final solution feasibility. The objective function was tailored to accommodate some \u201csoft\u201d constraints as well as penalize infeasible solutions (the earlier a solution was found to be infeasible, the larger the penalty). Because of the limited time at our disposal, we didn\u2019t attempt to tune the optimization algorithm.","title":"Tailoring process"},{"location":"examples/example_15/#what-was-tailored","text":"No response","title":"What was tailored"},{"location":"examples/example_15/#main-problem-characteristics","text":"Problem definition * 13 variables * 12 \u201ccontinuous\u201d (were discretized in evaluations, but were treated as continuous by the optimization algorithm) * 1 discrete (ordered, not combinatorial) * 1 objective (costs) * 2 \u201csoft\u201d constraints, whose violation was added to the main objective * 10 \u201chard\u201d constraints Other properties * The constraints were multimodal and hard to satisfy * While we were working on a single problem instance, the company had many instances to solve that could be handled in the same way * We could only afford to run the algorithm a handful of times, for a few thousand evaluations each * The code cannot be disclosed","title":"Main problem characteristics"},{"location":"examples/example_15/#references","text":"[Hansen2019] Nikolaus Hansen. 2019. A global surrogate assisted CMA-ES. V Proceedings of the 2019 Genetic and Evolutionary Computation Conference. ACM, New York, NY, USA, 664\u2013672. https://doi.org/10.1145/3321707.3321842","title":"References"},{"location":"examples/example_15/#contact-information-optional","text":"No response","title":"Contact information (optional)"},{"location":"examples/example_15/#author","text":"Tea Tu\u0161ar No response","title":"Author"},{"location":"examples/example_16/","text":"Energy System Optimization \u00b6 Problem Description \u00b6 Determine the optimal investment and operation decisions for different types of assets in the energy system (production, consumption, conversion, storage, and transport), while minimizing loss of load. It can support governments and network system operators. Why was tailoring needed? \u00b6 Tailoring was needed because of the scale of the problem: * Geographically: the power system in Europe is connected, allowing the use of generators in other countries when local generation is low (traded via energy markets). Investment decisions in one country are thus influenced by what happens in other countries. Decisions on investments in transmission capacity require an appropriate model of the high voltage network within each country. * Horizon: an investment in a new power plant has an impact for 30-50 years into the future. * Time resolution: because electricity used should be equal to electricity produced at all times, and renewable energy production may significantly change from one hour to the next, hourly resolution is needed. * Technical details: generation and storage have additional constraints (e.g. ramping and state of charge, respectively), which connect decisions on one time slot to the next. * Stochasticity: the computation is based on predictions of demand and weather/climate across the whole horizon, but we have no single perfect prediction for these, so need to take this uncertainty into account, e.g. by taking into account multiple scenarios. * Ideally multiple solutions are presented, because of unmodeled preferences * Ideally also the time at which the investments should be made is determined, but we usually just use one or two moments for this Baseline algorithm \u00b6 The default is to model this as a single mixed-integer linear program, because many decisions are binary (e.g. use a certain production or not) and the objective and some constraints are naturally linear (e.g. balance in the system), while others can be reasonably approximated with a linear constraint (e.g. power flow). Tailoring process \u00b6 Mainly we have been tailoring the model: * not use integer / binary variables, so the problem can be represented as an LP * From multi-objective to single-objective: although minimizing loss of load (i.e., not meeting demand) is a separate objective to minimizing investment costs and minimizing operational costs, these are all modeled as a single objective. * reformulate constraints: more tight formulations (taken from different sources in the literature) * from stochastic to deterministic: use a single deterministic input for demand and weather (= renewable capacity) - this was still quite large, so we modeled future periods by a smaller set of representative periods \u2014 finding these requires some intelligent preprocessing of the data (still working on doing this for a stochastic model variant) * further reduce number of variables and constraints: * allow the user to decide on the geographical scale/details, time resolution, horizon, and technical details * enable the user to formulate flexible use of time resolution: different time resolutions for different locations (e.g. when deciding on investments for the Netherlands, have 4-hourly time resolution for countries far away) Not successful: * using (primal-dual) learning to predict the outcome or bounds of the operation subproblem/simulation * alternatives for finding good representatives periods What was tailored \u00b6 No response Main problem characteristics \u00b6 dimensions: millions of variables and constraints (including the \u201csimulator\u201d) explicit knowledge of the constraints (the simulator is in fact making the decisions on hour-to-hour operations, which is modeled as part of the optimization problem) stochastic/uncertainty in input parameters (\u2026) more or less linear constraints and objective need to be able to explain decisions to a board of experts/directors, possibly government runtime budget: up to a week (but better would be the time to get a cup of coffee) References \u00b6 https://tulipaenergy.github.io/TulipaEnergyModel.jl/stable/40-scientific-foundation/45-scientific-references/ Contact information (optional) \u00b6 No response Author \u00b6 Mathijs de Weerdt No response","title":"Energy System Optimization"},{"location":"examples/example_16/#energy-system-optimization","text":"","title":"Energy System Optimization"},{"location":"examples/example_16/#problem-description","text":"Determine the optimal investment and operation decisions for different types of assets in the energy system (production, consumption, conversion, storage, and transport), while minimizing loss of load. It can support governments and network system operators.","title":"Problem Description"},{"location":"examples/example_16/#why-was-tailoring-needed","text":"Tailoring was needed because of the scale of the problem: * Geographically: the power system in Europe is connected, allowing the use of generators in other countries when local generation is low (traded via energy markets). Investment decisions in one country are thus influenced by what happens in other countries. Decisions on investments in transmission capacity require an appropriate model of the high voltage network within each country. * Horizon: an investment in a new power plant has an impact for 30-50 years into the future. * Time resolution: because electricity used should be equal to electricity produced at all times, and renewable energy production may significantly change from one hour to the next, hourly resolution is needed. * Technical details: generation and storage have additional constraints (e.g. ramping and state of charge, respectively), which connect decisions on one time slot to the next. * Stochasticity: the computation is based on predictions of demand and weather/climate across the whole horizon, but we have no single perfect prediction for these, so need to take this uncertainty into account, e.g. by taking into account multiple scenarios. * Ideally multiple solutions are presented, because of unmodeled preferences * Ideally also the time at which the investments should be made is determined, but we usually just use one or two moments for this","title":"Why was tailoring needed?"},{"location":"examples/example_16/#baseline-algorithm","text":"The default is to model this as a single mixed-integer linear program, because many decisions are binary (e.g. use a certain production or not) and the objective and some constraints are naturally linear (e.g. balance in the system), while others can be reasonably approximated with a linear constraint (e.g. power flow).","title":"Baseline algorithm"},{"location":"examples/example_16/#tailoring-process","text":"Mainly we have been tailoring the model: * not use integer / binary variables, so the problem can be represented as an LP * From multi-objective to single-objective: although minimizing loss of load (i.e., not meeting demand) is a separate objective to minimizing investment costs and minimizing operational costs, these are all modeled as a single objective. * reformulate constraints: more tight formulations (taken from different sources in the literature) * from stochastic to deterministic: use a single deterministic input for demand and weather (= renewable capacity) - this was still quite large, so we modeled future periods by a smaller set of representative periods \u2014 finding these requires some intelligent preprocessing of the data (still working on doing this for a stochastic model variant) * further reduce number of variables and constraints: * allow the user to decide on the geographical scale/details, time resolution, horizon, and technical details * enable the user to formulate flexible use of time resolution: different time resolutions for different locations (e.g. when deciding on investments for the Netherlands, have 4-hourly time resolution for countries far away) Not successful: * using (primal-dual) learning to predict the outcome or bounds of the operation subproblem/simulation * alternatives for finding good representatives periods","title":"Tailoring process"},{"location":"examples/example_16/#what-was-tailored","text":"No response","title":"What was tailored"},{"location":"examples/example_16/#main-problem-characteristics","text":"dimensions: millions of variables and constraints (including the \u201csimulator\u201d) explicit knowledge of the constraints (the simulator is in fact making the decisions on hour-to-hour operations, which is modeled as part of the optimization problem) stochastic/uncertainty in input parameters (\u2026) more or less linear constraints and objective need to be able to explain decisions to a board of experts/directors, possibly government runtime budget: up to a week (but better would be the time to get a cup of coffee)","title":"Main problem characteristics"},{"location":"examples/example_16/#references","text":"https://tulipaenergy.github.io/TulipaEnergyModel.jl/stable/40-scientific-foundation/45-scientific-references/","title":"References"},{"location":"examples/example_16/#contact-information-optional","text":"No response","title":"Contact information (optional)"},{"location":"examples/example_16/#author","text":"Mathijs de Weerdt No response","title":"Author"},{"location":"examples/example_17/","text":"Automatic Train Operation \u00b6 Problem Description \u00b6 Automatic Train Operation Pipeline. Optimizing parameter values of the modules in the automation pipeline, minimizing energy consumption & driving duration Why was tailoring needed? \u00b6 Baseline algorithm that was tailored (if it exists), motivation for choice: MO-SMAC (large search space and low budget), NSGA-II Baseline algorithm \u00b6 No response Tailoring process \u00b6 What was tailored (Problem model, mutation operator) Faster evaluations through a more efficient implementation Add penalty if solution is not applicable What was tailored \u00b6 No response Main problem characteristics \u00b6 10 dimensions Multi-objective (2 objectives) Mixed types of variables (continuous, discrete) Deterministic Evaluations (~5-30mins) References \u00b6 No response Contact information (optional) \u00b6 No response Author \u00b6 Carolin Mensendiek No response","title":"Automatic Train Operation"},{"location":"examples/example_17/#automatic-train-operation","text":"","title":"Automatic Train Operation"},{"location":"examples/example_17/#problem-description","text":"Automatic Train Operation Pipeline. Optimizing parameter values of the modules in the automation pipeline, minimizing energy consumption & driving duration","title":"Problem Description"},{"location":"examples/example_17/#why-was-tailoring-needed","text":"Baseline algorithm that was tailored (if it exists), motivation for choice: MO-SMAC (large search space and low budget), NSGA-II","title":"Why was tailoring needed?"},{"location":"examples/example_17/#baseline-algorithm","text":"No response","title":"Baseline algorithm"},{"location":"examples/example_17/#tailoring-process","text":"What was tailored (Problem model, mutation operator) Faster evaluations through a more efficient implementation Add penalty if solution is not applicable","title":"Tailoring process"},{"location":"examples/example_17/#what-was-tailored","text":"No response","title":"What was tailored"},{"location":"examples/example_17/#main-problem-characteristics","text":"10 dimensions Multi-objective (2 objectives) Mixed types of variables (continuous, discrete) Deterministic Evaluations (~5-30mins)","title":"Main problem characteristics"},{"location":"examples/example_17/#references","text":"No response","title":"References"},{"location":"examples/example_17/#contact-information-optional","text":"No response","title":"Contact information (optional)"},{"location":"examples/example_17/#author","text":"Carolin Mensendiek No response","title":"Author"},{"location":"examples/example_18/","text":"Closed-loop optimization methods \u00b6 Problem description \u00b6 During the period 2004-2009, I worked with biologists, analytical chemists, and food scientists, developing methods to optimize instruments, experimental protocols, or actual products experimentally, a setup sometimes also called closed-loop optimization. I co-authored quite a number of papers during this period, both in applications journals and in the evolutionary computation literature, and supervised the PhD thesis listed above. Some of these works are summarised and collected in the first reference above. It has a lot to do with tailoring (but also aiming for as much generality as possible). Why was tailoring needed? \u00b6 Tailoring is needed for a cluster of reasons in these kind of applications * Very blackbox nature of the problem * Limited budget / expensive * Possibility of experiments breaking equipment resulting in no evaluation\u2026 or worse * Experiments depend on resources that themselves depend on purchasing, storing or manufacturing consumables; construction of subparts of the solution which then becomes a fixed thing (possibly with reuse capability but also shelf-life); storing and planning how to use these partial solutions; scheduling of experimental staff time. * Experiments may be anything from one-at-a-time to very highly parallel, but that is dictated by the platform; it\u2019s not a free choice as it is in many simulation environments. Baseline algorithm \u00b6 We mostly tailored simple EAs, or EAs with niching, or multiobjective EAs. The motivation for this was the blackbox nature of the problems and our familiarity with these methods. However, we looked for generality of the tailoring methods/steps to all the characteristics listed above. Later on, we tailored Bayesian optimization methods like ParEGO. Tailoring process \u00b6 In this line of work, we have developed algorithms (or \u201ctailored\u201d methods), and theory (or technical motivation or principles) in the following related areas: Ephemeral resource constraints. This is where evaluations depend on resources. If the resources are not there, the experiment (evaluation) cannot be done. (They are a kind of dynamic constraint). We have classified many types and developed methods that work with different EAs. Safe optimization. This is where it is possible to permanently lose members of your EA population (e.g., consider the solutions are reconfigurable nano-robots and some of them may break irreplaceably) Heterogeneous objectives. Often in closed-loop problems, the objectives of a single solution cannot or may not be evaluated at the same time. (Some may be much cheaper or less time consuming to evaluate than others). So we need better asynchronous EAs or EMO methods to be designed. Our tailoring has included the incorporation of reinforcement learning methods, scheduling/planning methods, and other heuristics or procedures, into evolutionary and Bayesian optimization methods. What was tailored? \u00b6 No response Main problem characteristics \u00b6 No response References \u00b6 J. Knowles, \"Closed-loop evolutionary multiobjective optimization,\" in IEEE Computational Intelligence Magazine, vol. 4, no. 3, pp. 77-91, Aug. 2009, doi: 10.1109/MCI.2009.933095 . Allmendinger, Richard. Tuning evolutionary search for closed-loop optimization. The University of Manchester (United Kingdom), 2012. PDF Allmendinger, Richard, and Joshua D. Knowles. \"Evolutionary Search in Lethal Environments.\" In IJCCI (ECTA-FCTA), pp. 63-72. 2011. PDF Allmendinger, Richard, and Joshua Knowles. \"\u2018Hang on a minute\u2019: Investigations on the effects of delayed objective functions in multiobjective optimization.\" In International Conference on Evolutionary Multi-Criterion Optimization, pp. 6-20. Berlin, Heidelberg: Springer Berlin Heidelberg, 2013. PDF Allmendinger, Richard, and Joshua Knowles. \"Policy learning in resource-constrained optimization.\" In Proceedings of the 13th annual conference on Genetic and evolutionary computation, pp. 1971-1978. 2011. https://dl.acm.org/doi/pdf/10.1145/2001576.2001841 Richard Allmendinger, Joshua Knowles; On Handling Ephemeral Resource Constraints in Evolutionary Search. Evol Comput 2013; 21 (3): 497\u2013531. doi: https://doi.org/10.1162/EVCO_a_00097 Kim, Youngmin, Richard Allmendinger, and Manuel L\u00f3pez-Ib\u00e1\u00f1ez. \"Safe learning and optimization techniques: Towards a survey of the state of the art.\" In International Workshop on the Foundations of Trustworthy AI Integrating Learning, Optimization and Reasoning, pp. 123-139. Cham: Springer International Publishing, 2020. https://arxiv.org/pdf/2101.09505 Contact information (optional) \u00b6 Joshua Knowles","title":"Closed-loop optimization methods"},{"location":"examples/example_18/#closed-loop-optimization-methods","text":"","title":"Closed-loop optimization methods"},{"location":"examples/example_18/#problem-description","text":"During the period 2004-2009, I worked with biologists, analytical chemists, and food scientists, developing methods to optimize instruments, experimental protocols, or actual products experimentally, a setup sometimes also called closed-loop optimization. I co-authored quite a number of papers during this period, both in applications journals and in the evolutionary computation literature, and supervised the PhD thesis listed above. Some of these works are summarised and collected in the first reference above. It has a lot to do with tailoring (but also aiming for as much generality as possible).","title":"Problem description"},{"location":"examples/example_18/#why-was-tailoring-needed","text":"Tailoring is needed for a cluster of reasons in these kind of applications * Very blackbox nature of the problem * Limited budget / expensive * Possibility of experiments breaking equipment resulting in no evaluation\u2026 or worse * Experiments depend on resources that themselves depend on purchasing, storing or manufacturing consumables; construction of subparts of the solution which then becomes a fixed thing (possibly with reuse capability but also shelf-life); storing and planning how to use these partial solutions; scheduling of experimental staff time. * Experiments may be anything from one-at-a-time to very highly parallel, but that is dictated by the platform; it\u2019s not a free choice as it is in many simulation environments.","title":"Why was tailoring needed?"},{"location":"examples/example_18/#baseline-algorithm","text":"We mostly tailored simple EAs, or EAs with niching, or multiobjective EAs. The motivation for this was the blackbox nature of the problems and our familiarity with these methods. However, we looked for generality of the tailoring methods/steps to all the characteristics listed above. Later on, we tailored Bayesian optimization methods like ParEGO.","title":"Baseline algorithm"},{"location":"examples/example_18/#tailoring-process","text":"In this line of work, we have developed algorithms (or \u201ctailored\u201d methods), and theory (or technical motivation or principles) in the following related areas: Ephemeral resource constraints. This is where evaluations depend on resources. If the resources are not there, the experiment (evaluation) cannot be done. (They are a kind of dynamic constraint). We have classified many types and developed methods that work with different EAs. Safe optimization. This is where it is possible to permanently lose members of your EA population (e.g., consider the solutions are reconfigurable nano-robots and some of them may break irreplaceably) Heterogeneous objectives. Often in closed-loop problems, the objectives of a single solution cannot or may not be evaluated at the same time. (Some may be much cheaper or less time consuming to evaluate than others). So we need better asynchronous EAs or EMO methods to be designed. Our tailoring has included the incorporation of reinforcement learning methods, scheduling/planning methods, and other heuristics or procedures, into evolutionary and Bayesian optimization methods.","title":"Tailoring process"},{"location":"examples/example_18/#what-was-tailored","text":"No response","title":"What was tailored?"},{"location":"examples/example_18/#main-problem-characteristics","text":"No response","title":"Main problem characteristics"},{"location":"examples/example_18/#references","text":"J. Knowles, \"Closed-loop evolutionary multiobjective optimization,\" in IEEE Computational Intelligence Magazine, vol. 4, no. 3, pp. 77-91, Aug. 2009, doi: 10.1109/MCI.2009.933095 . Allmendinger, Richard. Tuning evolutionary search for closed-loop optimization. The University of Manchester (United Kingdom), 2012. PDF Allmendinger, Richard, and Joshua D. Knowles. \"Evolutionary Search in Lethal Environments.\" In IJCCI (ECTA-FCTA), pp. 63-72. 2011. PDF Allmendinger, Richard, and Joshua Knowles. \"\u2018Hang on a minute\u2019: Investigations on the effects of delayed objective functions in multiobjective optimization.\" In International Conference on Evolutionary Multi-Criterion Optimization, pp. 6-20. Berlin, Heidelberg: Springer Berlin Heidelberg, 2013. PDF Allmendinger, Richard, and Joshua Knowles. \"Policy learning in resource-constrained optimization.\" In Proceedings of the 13th annual conference on Genetic and evolutionary computation, pp. 1971-1978. 2011. https://dl.acm.org/doi/pdf/10.1145/2001576.2001841 Richard Allmendinger, Joshua Knowles; On Handling Ephemeral Resource Constraints in Evolutionary Search. Evol Comput 2013; 21 (3): 497\u2013531. doi: https://doi.org/10.1162/EVCO_a_00097 Kim, Youngmin, Richard Allmendinger, and Manuel L\u00f3pez-Ib\u00e1\u00f1ez. \"Safe learning and optimization techniques: Towards a survey of the state of the art.\" In International Workshop on the Foundations of Trustworthy AI Integrating Learning, Optimization and Reasoning, pp. 123-139. Cham: Springer International Publishing, 2020. https://arxiv.org/pdf/2101.09505","title":"References"},{"location":"examples/example_18/#contact-information-optional","text":"Joshua Knowles","title":"Contact information (optional)"},{"location":"examples/example_19/","text":"Bayesian Optimization with Gaussian Processes for Protein Design \u00b6 Problem Description \u00b6 From a pool of candidate protein sequences, find the best one. The general search space is mutating an amino acid in the sequence. We only target mutating one protein wild type. Why was tailoring needed? \u00b6 Representations could be very long, normally, proteins are represented as the sequence (could get up to 700 dimensions). Some form of prior knowledge in the form of protein language models available, so we aim to include this to speed up the optimization. Baseline algorithm \u00b6 BO with a GP and specific fingerprint/string kernels. Tailoring process \u00b6 Identify possible modeling obstacles when using a GP. Identify prior knowledge. What was tailored \u00b6 Adapt the representation/encoding: From the raw string representation of the protein as a sequence of amino acids, represent the variant as the mutation code. We only look at single mutations. This reduces the feature vector/dimensions from up to 700 to 3, which is much better consumable by the GP. In addition, by inspecting the protein landscapes from datasets, it is evident, that the location of the mutation plays a major role. Encoding the location directly should assist optimization. Use the protein language model's predictions as the prior mean to prime the mode. Interpolate between this prior and a standard constant mean, and adapt this interpolation coefficient based on the observed data (simply included in the fitting process). Main problem characteristics \u00b6 combinatorial, high-dimensional, protein design References \u00b6 Abstract: https://scholar.google.com/citations?view_op=view_citation&hl=de&user=nx_Xq8YAAAAJ&citation_for_view=nx_Xq8YAAAAJ:8k81kl-MbHgC Contact information (optional) \u00b6 No response Author \u00b6 Carolin Benjamins No response","title":"Bayesian Optimization with Gaussian Processes for Protein Design"},{"location":"examples/example_19/#bayesian-optimization-with-gaussian-processes-for-protein-design","text":"","title":"Bayesian Optimization with Gaussian Processes for Protein Design"},{"location":"examples/example_19/#problem-description","text":"From a pool of candidate protein sequences, find the best one. The general search space is mutating an amino acid in the sequence. We only target mutating one protein wild type.","title":"Problem Description"},{"location":"examples/example_19/#why-was-tailoring-needed","text":"Representations could be very long, normally, proteins are represented as the sequence (could get up to 700 dimensions). Some form of prior knowledge in the form of protein language models available, so we aim to include this to speed up the optimization.","title":"Why was tailoring needed?"},{"location":"examples/example_19/#baseline-algorithm","text":"BO with a GP and specific fingerprint/string kernels.","title":"Baseline algorithm"},{"location":"examples/example_19/#tailoring-process","text":"Identify possible modeling obstacles when using a GP. Identify prior knowledge.","title":"Tailoring process"},{"location":"examples/example_19/#what-was-tailored","text":"Adapt the representation/encoding: From the raw string representation of the protein as a sequence of amino acids, represent the variant as the mutation code. We only look at single mutations. This reduces the feature vector/dimensions from up to 700 to 3, which is much better consumable by the GP. In addition, by inspecting the protein landscapes from datasets, it is evident, that the location of the mutation plays a major role. Encoding the location directly should assist optimization. Use the protein language model's predictions as the prior mean to prime the mode. Interpolate between this prior and a standard constant mean, and adapt this interpolation coefficient based on the observed data (simply included in the fitting process).","title":"What was tailored"},{"location":"examples/example_19/#main-problem-characteristics","text":"combinatorial, high-dimensional, protein design","title":"Main problem characteristics"},{"location":"examples/example_19/#references","text":"Abstract: https://scholar.google.com/citations?view_op=view_citation&hl=de&user=nx_Xq8YAAAAJ&citation_for_view=nx_Xq8YAAAAJ:8k81kl-MbHgC","title":"References"},{"location":"examples/example_19/#contact-information-optional","text":"No response","title":"Contact information (optional)"},{"location":"examples/example_19/#author","text":"Carolin Benjamins No response","title":"Author"},{"location":"examples/example_2/","text":"Building spatial design \u00b6 Problem Description \u00b6 Optimise the spatial layout of a building to Minimise energy consumption for climate control, and Minimise the strain on the structure Why was tailoring needed? \u00b6 Many infeasible solutions exist. No algorithm existed/was found that could handle both multiple objectives, and a mixed-variable search space. Baseline algorithm \u00b6 SMS-EMOA. This performed best after initial tailoring steps needed to be able to run an algorithm at all, that were applied to both SMS-EMOA and NSGA-II. Tailoring process \u00b6 Design a superstructure problem representation to ensure the number of variables stays fixed (the existing representation would change the number of variables regularly). This made it possible to use standard EA frameworks . Modify algorithms designed for a single variable type to handle two variable types. ( Add existing standard mutation + crossover operators to handle variable types the algorithm cannot handle natively.) This made it possible to run an algorithm at all. Add equal penalty value for any solution that is infeasible . This led to only a very small number of feasible solutions, and not much optimisation yet. Penalty value based on the number of constraint violations (larger penalty for more violations). This led to more feasible solutions, and actually being able to optimise something. Design a problem-specific initialisation operator to ensure all initial solutions are feasible. This, combined with the next three steps, led to much better Pareto front approximations. Design a problem-specific mutation operator for the binary variables to ensure (standard operator was used for continuous variables, since there were no constraints on those) Remove the standard crossover operator , because it would cause many constraint violations (a new problem-specific crossover operator might be designed later, but proved to be very difficult to do) Add repair function was used to proportionally scale the continuous variables after mutation to match an equality constraint on them. Tune algorithm parameters to further improve performance. This led to further Pareto front approximation improvements. Add a local search step to try to improve solutions further by fixing the binary variables and optimising the continuous variables within that subspace.. This did not make much of a difference in the quality of the Pareto front approximation. Partial success : Although the developed approach worked in principle, it was quite limited in the size of the designs it could handle. For larger designs more modifications would be needed to ensure the problem-specific operators would not get stuck in a (fairly) local region of the search space. This was because the probability of finding a feasible mutation decreased when more modifications were made to the parent. (For small enough designs, the whole space would be local enough for things to work.) What was tailored \u00b6 No response Main problem characteristics \u00b6 Many hard constraints (simulator cannot evaluate the solution if these are violated) / Large part of the search space was infeasible. Checking if / how many constraints are violated is cheap. Mixed-variable search space (continuous + binary) Multiple objectives (Somewhat) expensive solution evaluations; with larger designs being more expensive. E.g., rough 1 second per evaluation for the smallest considered design, and roughly 40 seconds for the larger designs we considered. (Even the larger designs we considered are still relatively small for the considered problem.) Because of the above, we restricted ourselves to 2500 evaluations, but this was not a strict requirement. References \u00b6 No response Author \u00b6 Koen van der Blom","title":"Building spatial design"},{"location":"examples/example_2/#building-spatial-design","text":"","title":"Building spatial design"},{"location":"examples/example_2/#problem-description","text":"Optimise the spatial layout of a building to Minimise energy consumption for climate control, and Minimise the strain on the structure","title":"Problem Description"},{"location":"examples/example_2/#why-was-tailoring-needed","text":"Many infeasible solutions exist. No algorithm existed/was found that could handle both multiple objectives, and a mixed-variable search space.","title":"Why was tailoring needed?"},{"location":"examples/example_2/#baseline-algorithm","text":"SMS-EMOA. This performed best after initial tailoring steps needed to be able to run an algorithm at all, that were applied to both SMS-EMOA and NSGA-II.","title":"Baseline algorithm"},{"location":"examples/example_2/#tailoring-process","text":"Design a superstructure problem representation to ensure the number of variables stays fixed (the existing representation would change the number of variables regularly). This made it possible to use standard EA frameworks . Modify algorithms designed for a single variable type to handle two variable types. ( Add existing standard mutation + crossover operators to handle variable types the algorithm cannot handle natively.) This made it possible to run an algorithm at all. Add equal penalty value for any solution that is infeasible . This led to only a very small number of feasible solutions, and not much optimisation yet. Penalty value based on the number of constraint violations (larger penalty for more violations). This led to more feasible solutions, and actually being able to optimise something. Design a problem-specific initialisation operator to ensure all initial solutions are feasible. This, combined with the next three steps, led to much better Pareto front approximations. Design a problem-specific mutation operator for the binary variables to ensure (standard operator was used for continuous variables, since there were no constraints on those) Remove the standard crossover operator , because it would cause many constraint violations (a new problem-specific crossover operator might be designed later, but proved to be very difficult to do) Add repair function was used to proportionally scale the continuous variables after mutation to match an equality constraint on them. Tune algorithm parameters to further improve performance. This led to further Pareto front approximation improvements. Add a local search step to try to improve solutions further by fixing the binary variables and optimising the continuous variables within that subspace.. This did not make much of a difference in the quality of the Pareto front approximation. Partial success : Although the developed approach worked in principle, it was quite limited in the size of the designs it could handle. For larger designs more modifications would be needed to ensure the problem-specific operators would not get stuck in a (fairly) local region of the search space. This was because the probability of finding a feasible mutation decreased when more modifications were made to the parent. (For small enough designs, the whole space would be local enough for things to work.)","title":"Tailoring process"},{"location":"examples/example_2/#what-was-tailored","text":"No response","title":"What was tailored"},{"location":"examples/example_2/#main-problem-characteristics","text":"Many hard constraints (simulator cannot evaluate the solution if these are violated) / Large part of the search space was infeasible. Checking if / how many constraints are violated is cheap. Mixed-variable search space (continuous + binary) Multiple objectives (Somewhat) expensive solution evaluations; with larger designs being more expensive. E.g., rough 1 second per evaluation for the smallest considered design, and roughly 40 seconds for the larger designs we considered. (Even the larger designs we considered are still relatively small for the considered problem.) Because of the above, we restricted ourselves to 2500 evaluations, but this was not a strict requirement.","title":"Main problem characteristics"},{"location":"examples/example_2/#references","text":"No response","title":"References"},{"location":"examples/example_2/#author","text":"Koen van der Blom","title":"Author"},{"location":"examples/example_3/","text":"Combined Topology and Fibre-Orientation via Moving Morphable Components and Lamination Parameters \u00b6 Problem Description \u00b6 The idea is to optimize the topology or layout of a structure and the internal material layout. The topology was modelled via Moving Morphable Components (MMCs) wherein prescribed shapes are used to deform and move throughout a design 2D space. On the other hand, the fiber orientation was optimized by using the Lamination Parameter Formulation, which requires setting two parameters with global effects at some master nodes and the local fiber orientations are found by interpolation of these two parameters with respect to proximality to the master nodes. Why was tailoring needed? \u00b6 Normally, Topology Optimization requires more than 1000 design variables in most used formulations for this matter. Therefore, the Moving Morphable Components formulation was used to reduce dimensionality in a way that is tractable for black-box optimizers, but paying the cost of a much more reduced solution space. Simulation malfunctions required to be included as \u201cobjective constraints\u201d since some design combinations are numerically infeasible in the way the structure became kinematic and the underlying boundary conditions of the Finite Element solver weren\u2019t fulfilled. By evaluating the target for those sections of the search space, the algorithms get stuck since the target values are uninformative. For surrogate assisted optimization this is a big problem due to spurious correlations and discontinuities. Baseline algorithm \u00b6 No response Tailoring process \u00b6 Changed the target into a piecewise definition by evaluating first if the underlying structure complied with the boundary conditions. In other words, at least there was material connecting the structure from the support and there was material next to the load application. We adapted the function to handle constraints in an Augmented Lagrangian Fashion. We observed not so many works in the intersection of small feasible regions of search spaces and high-dimensional settings. However this may be counterproductive for Bayesian Optimization due to the discontinuity of the intersection of the feasible and unfeasible regions. What was tailored \u00b6 No response Main problem characteristics \u00b6 Continuous formulation of an inherent combinatorial high-dimensional space. Small feasible region or highly constrained Not all the search space can be evaluated and are meaningful. References \u00b6 No response Author \u00b6 Iv\u00e1n Olarte Rodr\u00edguez, Gokhan Serhat, Mariusz Bujny, Thomas Baeck, Elena Raponi","title":"Combined Topology and Fibre-Orientation via Moving Morphable Components and Lamination Parameters"},{"location":"examples/example_3/#combined-topology-and-fibre-orientation-via-moving-morphable-components-and-lamination-parameters","text":"","title":"Combined Topology and Fibre-Orientation via Moving Morphable Components and Lamination Parameters"},{"location":"examples/example_3/#problem-description","text":"The idea is to optimize the topology or layout of a structure and the internal material layout. The topology was modelled via Moving Morphable Components (MMCs) wherein prescribed shapes are used to deform and move throughout a design 2D space. On the other hand, the fiber orientation was optimized by using the Lamination Parameter Formulation, which requires setting two parameters with global effects at some master nodes and the local fiber orientations are found by interpolation of these two parameters with respect to proximality to the master nodes.","title":"Problem Description"},{"location":"examples/example_3/#why-was-tailoring-needed","text":"Normally, Topology Optimization requires more than 1000 design variables in most used formulations for this matter. Therefore, the Moving Morphable Components formulation was used to reduce dimensionality in a way that is tractable for black-box optimizers, but paying the cost of a much more reduced solution space. Simulation malfunctions required to be included as \u201cobjective constraints\u201d since some design combinations are numerically infeasible in the way the structure became kinematic and the underlying boundary conditions of the Finite Element solver weren\u2019t fulfilled. By evaluating the target for those sections of the search space, the algorithms get stuck since the target values are uninformative. For surrogate assisted optimization this is a big problem due to spurious correlations and discontinuities.","title":"Why was tailoring needed?"},{"location":"examples/example_3/#baseline-algorithm","text":"No response","title":"Baseline algorithm"},{"location":"examples/example_3/#tailoring-process","text":"Changed the target into a piecewise definition by evaluating first if the underlying structure complied with the boundary conditions. In other words, at least there was material connecting the structure from the support and there was material next to the load application. We adapted the function to handle constraints in an Augmented Lagrangian Fashion. We observed not so many works in the intersection of small feasible regions of search spaces and high-dimensional settings. However this may be counterproductive for Bayesian Optimization due to the discontinuity of the intersection of the feasible and unfeasible regions.","title":"Tailoring process"},{"location":"examples/example_3/#what-was-tailored","text":"No response","title":"What was tailored"},{"location":"examples/example_3/#main-problem-characteristics","text":"Continuous formulation of an inherent combinatorial high-dimensional space. Small feasible region or highly constrained Not all the search space can be evaluated and are meaningful.","title":"Main problem characteristics"},{"location":"examples/example_3/#references","text":"No response","title":"References"},{"location":"examples/example_3/#author","text":"Iv\u00e1n Olarte Rodr\u00edguez, Gokhan Serhat, Mariusz Bujny, Thomas Baeck, Elena Raponi","title":"Author"},{"location":"examples/example_4/","text":"Antenna control \u00b6 Problem Description \u00b6 tune a phased array antenna to steer it towards a target satellite, for telecommunication Why was tailoring needed? \u00b6 existing algorithms were either too inefficient, or required too many function evaluations Baseline algorithm \u00b6 surrogate-based optimization. To reduce the required number of function evaluations. Tailoring process \u00b6 The goal was to develop an algorithm that is efficient in the number of required function evaluations, but also in the computation time needed to propose a new candidate solution. The chosen framework was the same as in surrogate-based optimization: start with a candidate solution, evaluate the solution, update a surrogate model on the data gathered so far, then use the surrogate model to guide the search towards the optimal solution. The idea was that this framework would need less function evaluations than other black-box optimization algorithms such as evolutionary algorithms, though this was not tested. The chosen surrogate model was a Random Fourier Expansion, as it is efficient to train and to update, does not slow down over time like Gaussian processes, and has theoretical guarantees available. The chosen acquisition function was a type of epsilon-greedy exploration with local perturbations. This allowed for a fast convergence towards a local optimum, while not getting stuck at a local optimum. It avoided the need of spending computation time on calculating covariances or optimizing surrogate model hyperparameters. Hyperparameters of the approach were chosen based on expertise and initial experiments. For example, the number of basis functions was a hyperparameter that needed to be chosen such that it balances computational efficiency and performance. What was not successful: Nelder-Mead simplex method Powell\u2019s method using a quadratic surrogate model same as proposed approach but pure greedy (epsilon=0) \u2192 got stuck in local optima Bayesian optimization with Gaussian processes a more white-box approach that tried to linearize the problem Result good enough solution found within 2 minutes, with 3000 function evaluations solution 3000 times faster than Bayesian optimization with Gaussian processes solution twice as good as a predict-then-optimize approach with the same surrogate model What was tailored \u00b6 an algorithm was developed from scratch, with the same framework as other surrogate-based optimization algorithms. Main problem characteristics \u00b6 24 continuous variables with lower and upper bounds defined and no other constraints relatively cheap but noisy objective black-box, single-objective, single-fidelity, no parallelization Time constraint of 2 minutes (8 years ago, maybe now it would be 1 second or something) Function evaluation constraint of 3000 tested on a simulator, with the goal of eventually being applied on a physical system References \u00b6 optimal beam-forming network tuning more information on the application (not the algorithm) Contact information (optional) \u00b6 No response Author \u00b6 Laurens Bliek No response","title":"Antenna control"},{"location":"examples/example_4/#antenna-control","text":"","title":"Antenna control"},{"location":"examples/example_4/#problem-description","text":"tune a phased array antenna to steer it towards a target satellite, for telecommunication","title":"Problem Description"},{"location":"examples/example_4/#why-was-tailoring-needed","text":"existing algorithms were either too inefficient, or required too many function evaluations","title":"Why was tailoring needed?"},{"location":"examples/example_4/#baseline-algorithm","text":"surrogate-based optimization. To reduce the required number of function evaluations.","title":"Baseline algorithm"},{"location":"examples/example_4/#tailoring-process","text":"The goal was to develop an algorithm that is efficient in the number of required function evaluations, but also in the computation time needed to propose a new candidate solution. The chosen framework was the same as in surrogate-based optimization: start with a candidate solution, evaluate the solution, update a surrogate model on the data gathered so far, then use the surrogate model to guide the search towards the optimal solution. The idea was that this framework would need less function evaluations than other black-box optimization algorithms such as evolutionary algorithms, though this was not tested. The chosen surrogate model was a Random Fourier Expansion, as it is efficient to train and to update, does not slow down over time like Gaussian processes, and has theoretical guarantees available. The chosen acquisition function was a type of epsilon-greedy exploration with local perturbations. This allowed for a fast convergence towards a local optimum, while not getting stuck at a local optimum. It avoided the need of spending computation time on calculating covariances or optimizing surrogate model hyperparameters. Hyperparameters of the approach were chosen based on expertise and initial experiments. For example, the number of basis functions was a hyperparameter that needed to be chosen such that it balances computational efficiency and performance. What was not successful: Nelder-Mead simplex method Powell\u2019s method using a quadratic surrogate model same as proposed approach but pure greedy (epsilon=0) \u2192 got stuck in local optima Bayesian optimization with Gaussian processes a more white-box approach that tried to linearize the problem Result good enough solution found within 2 minutes, with 3000 function evaluations solution 3000 times faster than Bayesian optimization with Gaussian processes solution twice as good as a predict-then-optimize approach with the same surrogate model","title":"Tailoring process"},{"location":"examples/example_4/#what-was-tailored","text":"an algorithm was developed from scratch, with the same framework as other surrogate-based optimization algorithms.","title":"What was tailored"},{"location":"examples/example_4/#main-problem-characteristics","text":"24 continuous variables with lower and upper bounds defined and no other constraints relatively cheap but noisy objective black-box, single-objective, single-fidelity, no parallelization Time constraint of 2 minutes (8 years ago, maybe now it would be 1 second or something) Function evaluation constraint of 3000 tested on a simulator, with the goal of eventually being applied on a physical system","title":"Main problem characteristics"},{"location":"examples/example_4/#references","text":"optimal beam-forming network tuning more information on the application (not the algorithm)","title":"References"},{"location":"examples/example_4/#contact-information-optional","text":"No response","title":"Contact information (optional)"},{"location":"examples/example_4/#author","text":"Laurens Bliek No response","title":"Author"},{"location":"examples/example_5/","text":"Deep RL in Ludii (general game playing) \u00b6 Problem Description \u00b6 learning to play board games ( policies and/or value functions ) in the Ludii system: a single system with ~ 1400 distinct games described in its custom game description language (a DSL). Ludii compiles game description files into runnable simulators. Why was tailoring needed? \u00b6 biquitous assumption in pretty much the entire Deep RL field is that, when we have a domain (e.g., a particular game), we can precisely enumerate or define the whole action space. This is necessary for the established approach (which the entire field follows) for discrete-action-space tasks, where the number of output nodes of a neural network is equal to the number of actions. This is impossible in Ludii: from a game description file, I cannot automatically infer the action space of the entire game a priori. I can only, given a current state, generate a list of legal moves for that particular state . Hence: don\u2019t even know how many output nodes my neural network needs . Baseline algorithm \u00b6 AlphaZero-like training of a neural network with policy and state value heads, combined with MCTS. Tailoring process \u00b6 we could make a reasonable approximation of the action space that works okay for many games. But there are new risks that were never considered as possibilities by established deep RL methodologies: we sometimes have multiple different actions mapping to a single output node in the policy head. This raises questions about how to train that node (if one action is really good but the other really bad, should the output node have a high or low probability? Probably naively going in between is not great). There is also interesting interaction with the MCTS component, because it searches for a specific state at a time, and actually is able to distinguish between the actions that a DNN cannot distinguish between. Alternative solution is to include an actual feature-based representation of actions, and swap the action over to the input instead of the output. But then we need one forwards pass per action per state, instead of just one per state (giving outputs for all actions in parallel). Only explored this with linear functions, not DNNs. What was tailored \u00b6 problem description was tailored in the sense that we really want to use Ludii (no other system exists with anywhere close to as many different games). Given that constraint, tailoring of neural network architecture, training algorithm, and integration with search became necessary. Main problem characteristics \u00b6 Sequential decision-making. Discrete action space. Lots of problem instances (~1400 distinct board games) described in a single DSL. Somewhat inefficient simulator (due to need to compile from DSL). No automated inferencing (DSL is not logic-based). References \u00b6 No response Contact information (optional) \u00b6 No response Author \u00b6 Dennis Soemers No response","title":"Deep RL in Ludii (general game playing)"},{"location":"examples/example_5/#deep-rl-in-ludii-general-game-playing","text":"","title":"Deep RL in Ludii (general game playing)"},{"location":"examples/example_5/#problem-description","text":"learning to play board games ( policies and/or value functions ) in the Ludii system: a single system with ~ 1400 distinct games described in its custom game description language (a DSL). Ludii compiles game description files into runnable simulators.","title":"Problem Description"},{"location":"examples/example_5/#why-was-tailoring-needed","text":"biquitous assumption in pretty much the entire Deep RL field is that, when we have a domain (e.g., a particular game), we can precisely enumerate or define the whole action space. This is necessary for the established approach (which the entire field follows) for discrete-action-space tasks, where the number of output nodes of a neural network is equal to the number of actions. This is impossible in Ludii: from a game description file, I cannot automatically infer the action space of the entire game a priori. I can only, given a current state, generate a list of legal moves for that particular state . Hence: don\u2019t even know how many output nodes my neural network needs .","title":"Why was tailoring needed?"},{"location":"examples/example_5/#baseline-algorithm","text":"AlphaZero-like training of a neural network with policy and state value heads, combined with MCTS.","title":"Baseline algorithm"},{"location":"examples/example_5/#tailoring-process","text":"we could make a reasonable approximation of the action space that works okay for many games. But there are new risks that were never considered as possibilities by established deep RL methodologies: we sometimes have multiple different actions mapping to a single output node in the policy head. This raises questions about how to train that node (if one action is really good but the other really bad, should the output node have a high or low probability? Probably naively going in between is not great). There is also interesting interaction with the MCTS component, because it searches for a specific state at a time, and actually is able to distinguish between the actions that a DNN cannot distinguish between. Alternative solution is to include an actual feature-based representation of actions, and swap the action over to the input instead of the output. But then we need one forwards pass per action per state, instead of just one per state (giving outputs for all actions in parallel). Only explored this with linear functions, not DNNs.","title":"Tailoring process"},{"location":"examples/example_5/#what-was-tailored","text":"problem description was tailored in the sense that we really want to use Ludii (no other system exists with anywhere close to as many different games). Given that constraint, tailoring of neural network architecture, training algorithm, and integration with search became necessary.","title":"What was tailored"},{"location":"examples/example_5/#main-problem-characteristics","text":"Sequential decision-making. Discrete action space. Lots of problem instances (~1400 distinct board games) described in a single DSL. Somewhat inefficient simulator (due to need to compile from DSL). No automated inferencing (DSL is not logic-based).","title":"Main problem characteristics"},{"location":"examples/example_5/#references","text":"No response","title":"References"},{"location":"examples/example_5/#contact-information-optional","text":"No response","title":"Contact information (optional)"},{"location":"examples/example_5/#author","text":"Dennis Soemers No response","title":"Author"},{"location":"examples/example_6/","text":"MarioGAN \u00b6 Problem Description \u00b6 Generate (snippets of) Super Mario levels. Why was tailoring needed? \u00b6 Existing approaches usually use an encoding of the search space that maps tiles in the level directly to an entry in a matrix. This space then contains lots of unrealistic and unplayable levels, so it is unnecessarily big. We instead created a latent spaces that ideally only contains sensible levels and is easily searchable. Baseline algorithm \u00b6 Algorithm was not tailored, just the encoding. Since it was then turned into a single-objective continuous problem, we use cma-es (state-of-the-art) Tailoring process \u00b6 Colleagues had done latent variable search on another application, so we wanted to try it out here. There were some modifications later that changed the encoding further to prevent the creation of invalid levels (broken pipes). Lots of different objectives were also tried (since there is no clear given objective, it is a benchmarking problem) What was tailored \u00b6 No response Main problem characteristics \u00b6 Instances exist, search space is scalable Simulation-based evaluation with big spikes (adding one more tile to a pipe might make it unplayable) and large plateaus Not terribly expensive, but also not cheap Non-normal noise distribution (simulation of playthroughs is non-deterministic, and Mario tends to get stuck at specific points) References \u00b6 https://arxiv.org/abs/1805.00728 https://github.com/CIGbalance/DagstuhlGAN https://www.sciencedirect.com/science/article/pii/S1568494623001394 Contact information (optional) \u00b6 No response Author \u00b6 Vanessa Volz No response","title":"MarioGAN"},{"location":"examples/example_6/#mariogan","text":"","title":"MarioGAN"},{"location":"examples/example_6/#problem-description","text":"Generate (snippets of) Super Mario levels.","title":"Problem Description"},{"location":"examples/example_6/#why-was-tailoring-needed","text":"Existing approaches usually use an encoding of the search space that maps tiles in the level directly to an entry in a matrix. This space then contains lots of unrealistic and unplayable levels, so it is unnecessarily big. We instead created a latent spaces that ideally only contains sensible levels and is easily searchable.","title":"Why was tailoring needed?"},{"location":"examples/example_6/#baseline-algorithm","text":"Algorithm was not tailored, just the encoding. Since it was then turned into a single-objective continuous problem, we use cma-es (state-of-the-art)","title":"Baseline algorithm"},{"location":"examples/example_6/#tailoring-process","text":"Colleagues had done latent variable search on another application, so we wanted to try it out here. There were some modifications later that changed the encoding further to prevent the creation of invalid levels (broken pipes). Lots of different objectives were also tried (since there is no clear given objective, it is a benchmarking problem)","title":"Tailoring process"},{"location":"examples/example_6/#what-was-tailored","text":"No response","title":"What was tailored"},{"location":"examples/example_6/#main-problem-characteristics","text":"Instances exist, search space is scalable Simulation-based evaluation with big spikes (adding one more tile to a pipe might make it unplayable) and large plateaus Not terribly expensive, but also not cheap Non-normal noise distribution (simulation of playthroughs is non-deterministic, and Mario tends to get stuck at specific points)","title":"Main problem characteristics"},{"location":"examples/example_6/#references","text":"https://arxiv.org/abs/1805.00728 https://github.com/CIGbalance/DagstuhlGAN https://www.sciencedirect.com/science/article/pii/S1568494623001394","title":"References"},{"location":"examples/example_6/#contact-information-optional","text":"No response","title":"Contact information (optional)"},{"location":"examples/example_6/#author","text":"Vanessa Volz No response","title":"Author"},{"location":"examples/example_7/","text":"Multi-objective optimisation of a Stator \u00b6 Problem Description \u00b6 The problem involved three objectives and 20 decision variables. The first formulation came from the engineer in the company. We applied our in-house BO algorithm to solve it. After several discussions, we found out it was not a 20 but nearly 50 dimensional problem because of an additional mechanical system in the optimisation process. But we did not step here. Another engineer suggested that he could simplify the problem and combine two systems (with physics magic) and use only 15 decision variables. We ended up with three objectives and 15 decision variables. Why was tailoring needed? \u00b6 Tailoring was needed in the problem formulation and not in the algorithm. The tailoring modified the problem formulation and we hoped that it would result in a better reflection of the real-world problem. Baseline algorithm \u00b6 In-house Multi-objective BO Tailoring process \u00b6 Mostly discussions after solving the first problem formulation. In the future, we may end up with a different problem formulation. Challenges: Different terminology, converting decision-makers' experience, knowledge and their needs into an appropriate optimisation formulation What was tailored \u00b6 No response Main problem characteristics \u00b6 Multi-objective very expensive simulations medium dimensionality parallelisation hard black-box constraint handling References \u00b6 No response Contact information (optional) \u00b6 No response Author \u00b6 Tinkle Chugh No response","title":"Multi-objective optimisation of a Stator"},{"location":"examples/example_7/#multi-objective-optimisation-of-a-stator","text":"","title":"Multi-objective optimisation of a Stator"},{"location":"examples/example_7/#problem-description","text":"The problem involved three objectives and 20 decision variables. The first formulation came from the engineer in the company. We applied our in-house BO algorithm to solve it. After several discussions, we found out it was not a 20 but nearly 50 dimensional problem because of an additional mechanical system in the optimisation process. But we did not step here. Another engineer suggested that he could simplify the problem and combine two systems (with physics magic) and use only 15 decision variables. We ended up with three objectives and 15 decision variables.","title":"Problem Description"},{"location":"examples/example_7/#why-was-tailoring-needed","text":"Tailoring was needed in the problem formulation and not in the algorithm. The tailoring modified the problem formulation and we hoped that it would result in a better reflection of the real-world problem.","title":"Why was tailoring needed?"},{"location":"examples/example_7/#baseline-algorithm","text":"In-house Multi-objective BO","title":"Baseline algorithm"},{"location":"examples/example_7/#tailoring-process","text":"Mostly discussions after solving the first problem formulation. In the future, we may end up with a different problem formulation. Challenges: Different terminology, converting decision-makers' experience, knowledge and their needs into an appropriate optimisation formulation","title":"Tailoring process"},{"location":"examples/example_7/#what-was-tailored","text":"No response","title":"What was tailored"},{"location":"examples/example_7/#main-problem-characteristics","text":"Multi-objective very expensive simulations medium dimensionality parallelisation hard black-box constraint handling","title":"Main problem characteristics"},{"location":"examples/example_7/#references","text":"No response","title":"References"},{"location":"examples/example_7/#contact-information-optional","text":"No response","title":"Contact information (optional)"},{"location":"examples/example_7/#author","text":"Tinkle Chugh No response","title":"Author"},{"location":"examples/example_8/","text":"Optimizing Container turnover rate for a container yard by tuning parameters of a blackbox decision support system \u00b6 Problem Description \u00b6 Company running a container yard in port of Antwerp decides where to put incoming containers on the yard based upon a black-box decision support system. The system requires 6 parameters as input, and then decides on the best spot for each container coming in (based on weight, port of destination,...). The input parameters have been fixed for a long time, and have been decided upon using some rough-cut search. The location decisions have an important impact on the container turnover rate (= the number of containers that can be handled per hour by the cranes used for put-away/pick-up of containers). The company wishes to optimize the inputs in order to maximize the resulting container turnover rate. A simulation model (expensive in run time) is available to evaluate how a change in inputs affects the outcome. Why was tailoring needed? \u00b6 Very flat response surface, 6 dimensional space, optimizing MEI using local optimization from different starting points BUT no significant improvements Baseline algorithm \u00b6 LHS design (20 points) as starting design; BO with stochastic kriging (GP with heterogenous noise) as surrogate model and Modified Expected Improvement (MEI) as AF Heterogenous noise present (but rather small), number of dimensions doable for GP, continuous inputs and output Tailoring process \u00b6 triangulation points: explodes! So not very useful for this nr of dimensions pattern search = very useful What was tailored \u00b6 No response Main problem characteristics \u00b6 Heterogenous noise 6 dimensions continuous search space References \u00b6 No response Contact information (optional) \u00b6 No response Author \u00b6 Inneke Van Nieuwenhuyse No response","title":"Optimizing Container turnover rate for a container yard by tuning parameters of a blackbox decision support system"},{"location":"examples/example_8/#optimizing-container-turnover-rate-for-a-container-yard-by-tuning-parameters-of-a-blackbox-decision-support-system","text":"","title":"Optimizing Container turnover rate for a container yard by tuning parameters of a blackbox decision support system"},{"location":"examples/example_8/#problem-description","text":"Company running a container yard in port of Antwerp decides where to put incoming containers on the yard based upon a black-box decision support system. The system requires 6 parameters as input, and then decides on the best spot for each container coming in (based on weight, port of destination,...). The input parameters have been fixed for a long time, and have been decided upon using some rough-cut search. The location decisions have an important impact on the container turnover rate (= the number of containers that can be handled per hour by the cranes used for put-away/pick-up of containers). The company wishes to optimize the inputs in order to maximize the resulting container turnover rate. A simulation model (expensive in run time) is available to evaluate how a change in inputs affects the outcome.","title":"Problem Description"},{"location":"examples/example_8/#why-was-tailoring-needed","text":"Very flat response surface, 6 dimensional space, optimizing MEI using local optimization from different starting points BUT no significant improvements","title":"Why was tailoring needed?"},{"location":"examples/example_8/#baseline-algorithm","text":"LHS design (20 points) as starting design; BO with stochastic kriging (GP with heterogenous noise) as surrogate model and Modified Expected Improvement (MEI) as AF Heterogenous noise present (but rather small), number of dimensions doable for GP, continuous inputs and output","title":"Baseline algorithm"},{"location":"examples/example_8/#tailoring-process","text":"triangulation points: explodes! So not very useful for this nr of dimensions pattern search = very useful","title":"Tailoring process"},{"location":"examples/example_8/#what-was-tailored","text":"No response","title":"What was tailored"},{"location":"examples/example_8/#main-problem-characteristics","text":"Heterogenous noise 6 dimensions continuous search space","title":"Main problem characteristics"},{"location":"examples/example_8/#references","text":"No response","title":"References"},{"location":"examples/example_8/#contact-information-optional","text":"No response","title":"Contact information (optional)"},{"location":"examples/example_8/#author","text":"Inneke Van Nieuwenhuyse No response","title":"Author"},{"location":"examples/example_9/","text":"HPO for a speaker diarization model for emergency medical systems \u00b6 Problem Description \u00b6 In real-life applications, such as in emergency medical system dialogues, multiple speakers interact in dynamic and noisy environments. Speech processing systems in these settings therefore focus on two main questions: (1) who is speaking at a given time? and (2) what is being said? The first goal is approached by Speaker Diarization systems. Within a collaboration with the Helmholtz Biomedical Institute and in the scope of a bigger project on AI in emergency medicine, a student created a speaker diarization system that would help parse the conversations of paramedics in the field. The optimization problem in our particular use-case was hyperparameter optimization of an open-source pre-trained speech embedding model PyAnnote (see below if this is really tailoring or not). Why was tailoring needed? \u00b6 According to the discussion about tailoring, this concrete example is either not considered to be proper tailoring (by some) or is considered the \u201clightest\u201d tailoring approach (by others). In this particular case, the baseline approach worked off-the-shelf already, namely PyAnnote could be fine-tuned on the real-world datasets of interest. The only \u201ctailoring\u201d was applying an automated HPO technique to find a setting yielding better overall accuracy of the model. Baseline algorithm \u00b6 We used the HPO facade of SMAC3 (BO with Sobol initial design, RF as surrogate, logEI as AF, and other specific design choices) to optimize HPs for the segmentation pipeline and for the clustering pipeline separately. We optimized for accuracy, i.e., we minimized the diarization error rate. Tailoring process \u00b6 Nothing was really tailored, pretty much just applied as is. Leading me to believe that as soon as we can say that we \u201capplied as is\u201d we are not in the tailoring domain anymore. What was tailored \u00b6 No response Main problem characteristics \u00b6 Mixed search space (discrete/continuous) Low-dimensional Medium to high evaluation cost (fine tuning can take hours, inference is a bit fast) Parallelisation possible References \u00b6 No response Contact information (optional) \u00b6 No response Author \u00b6 Anja Jankovic No response","title":"HPO for a speaker diarization model for emergency medical systems"},{"location":"examples/example_9/#hpo-for-a-speaker-diarization-model-for-emergency-medical-systems","text":"","title":"HPO for a speaker diarization model for emergency medical systems"},{"location":"examples/example_9/#problem-description","text":"In real-life applications, such as in emergency medical system dialogues, multiple speakers interact in dynamic and noisy environments. Speech processing systems in these settings therefore focus on two main questions: (1) who is speaking at a given time? and (2) what is being said? The first goal is approached by Speaker Diarization systems. Within a collaboration with the Helmholtz Biomedical Institute and in the scope of a bigger project on AI in emergency medicine, a student created a speaker diarization system that would help parse the conversations of paramedics in the field. The optimization problem in our particular use-case was hyperparameter optimization of an open-source pre-trained speech embedding model PyAnnote (see below if this is really tailoring or not).","title":"Problem Description"},{"location":"examples/example_9/#why-was-tailoring-needed","text":"According to the discussion about tailoring, this concrete example is either not considered to be proper tailoring (by some) or is considered the \u201clightest\u201d tailoring approach (by others). In this particular case, the baseline approach worked off-the-shelf already, namely PyAnnote could be fine-tuned on the real-world datasets of interest. The only \u201ctailoring\u201d was applying an automated HPO technique to find a setting yielding better overall accuracy of the model.","title":"Why was tailoring needed?"},{"location":"examples/example_9/#baseline-algorithm","text":"We used the HPO facade of SMAC3 (BO with Sobol initial design, RF as surrogate, logEI as AF, and other specific design choices) to optimize HPs for the segmentation pipeline and for the clustering pipeline separately. We optimized for accuracy, i.e., we minimized the diarization error rate.","title":"Baseline algorithm"},{"location":"examples/example_9/#tailoring-process","text":"Nothing was really tailored, pretty much just applied as is. Leading me to believe that as soon as we can say that we \u201capplied as is\u201d we are not in the tailoring domain anymore.","title":"Tailoring process"},{"location":"examples/example_9/#what-was-tailored","text":"No response","title":"What was tailored"},{"location":"examples/example_9/#main-problem-characteristics","text":"Mixed search space (discrete/continuous) Low-dimensional Medium to high evaluation cost (fine tuning can take hours, inference is a bit fast) Parallelisation possible","title":"Main problem characteristics"},{"location":"examples/example_9/#references","text":"No response","title":"References"},{"location":"examples/example_9/#contact-information-optional","text":"No response","title":"Contact information (optional)"},{"location":"examples/example_9/#author","text":"Anja Jankovic No response","title":"Author"}]}