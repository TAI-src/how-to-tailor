{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to How to tailor Research in optimization tends to focus on how to improve the design or configuration of their methods from a theoretical perspective or motivated by empirical studies of algorithm behavior on either specific individual problems or broader benchmarking problem suites. Our Dagstuhl Seminar takes a different perspective \u2013 we wish to improve the algorithm practitioner, arming them with a recipe book for improving any one of a broad class of optimization methods (black-box methods) on any given problem, particularly focusing on real-world problems. In order to establish such a recipe book, we must look to real-world problems and first understand how they arrive on the desk of an optimization practitioner. The formulation of the problem itself is often (though not always) \u201cmessy\u201d with many crucial details missing (such as what the objective(s) are, what the decision space is, and what the constraints are)! But even when such details are apparently in place, the practitioner must remember they are choices, and it will be well to understand who has made those choices and why. At a second level, one must understand how many times the optimization problem must be solved, with what frequency, on what machines, with what data and simulators, and so on. At another level, the practitioner must understand uncertainty in the problem, how the solution is represented, how it will be used in practice, what current solution methods are used, and how they were arrived at. By considering these and a large host of other factors, the practitioner can then understand how they might go about leveraging all that domain knowledge to tailor an optimization method for that problem to achieve the best possible tuned method. The challenge for us all is to be able to do this for any given problem and to have tailoring strategies that work well for any (or a wide range of) black-box methods. We know that such general tailoring methods exist in broad outline in the advice given in old textbooks [1], but it is a long time since they were updated for modern optimization methods and modern infrastructure, such as our modern methods of benchmarking algorithms. The purpose of this Dagstuhl Seminar is to audit the current experience that exists in the field, pool it, and start to describe that experience in a way that goes beyond anecdote (although anecdotes can be useful) and moves us toward advice or recipes that actually work in practice. To achieve this, we aim to bring together researchers from a diverse range of backgrounds involved in real-world optimization from the customer side, the algorithm design or theoretical side, or practitioners who have already experienced successes and failures in the endeavor of tailoring algorithms to problems. The format of our seminar will be structured so that we hear from the cross-section of experienced and inexperienced practitioners, algorithm designers and customers, right from the start. We will then self-organize to achieve our aim of sharing our pooled experience and shaping it into practical, searchable and up-to-date advice on leveraging domain knowledge. [1] L. Davis, Handbook of genetic algorithms. Chapman & Hall, London, 1991. Example Collection As a first step, we are aiming to collect examples. Please add your examples (see below).","title":"Welcome to How to tailor"},{"location":"#welcome-to-how-to-tailor","text":"Research in optimization tends to focus on how to improve the design or configuration of their methods from a theoretical perspective or motivated by empirical studies of algorithm behavior on either specific individual problems or broader benchmarking problem suites. Our Dagstuhl Seminar takes a different perspective \u2013 we wish to improve the algorithm practitioner, arming them with a recipe book for improving any one of a broad class of optimization methods (black-box methods) on any given problem, particularly focusing on real-world problems. In order to establish such a recipe book, we must look to real-world problems and first understand how they arrive on the desk of an optimization practitioner. The formulation of the problem itself is often (though not always) \u201cmessy\u201d with many crucial details missing (such as what the objective(s) are, what the decision space is, and what the constraints are)! But even when such details are apparently in place, the practitioner must remember they are choices, and it will be well to understand who has made those choices and why. At a second level, one must understand how many times the optimization problem must be solved, with what frequency, on what machines, with what data and simulators, and so on. At another level, the practitioner must understand uncertainty in the problem, how the solution is represented, how it will be used in practice, what current solution methods are used, and how they were arrived at. By considering these and a large host of other factors, the practitioner can then understand how they might go about leveraging all that domain knowledge to tailor an optimization method for that problem to achieve the best possible tuned method. The challenge for us all is to be able to do this for any given problem and to have tailoring strategies that work well for any (or a wide range of) black-box methods. We know that such general tailoring methods exist in broad outline in the advice given in old textbooks [1], but it is a long time since they were updated for modern optimization methods and modern infrastructure, such as our modern methods of benchmarking algorithms. The purpose of this Dagstuhl Seminar is to audit the current experience that exists in the field, pool it, and start to describe that experience in a way that goes beyond anecdote (although anecdotes can be useful) and moves us toward advice or recipes that actually work in practice. To achieve this, we aim to bring together researchers from a diverse range of backgrounds involved in real-world optimization from the customer side, the algorithm design or theoretical side, or practitioners who have already experienced successes and failures in the endeavor of tailoring algorithms to problems. The format of our seminar will be structured so that we hear from the cross-section of experienced and inexperienced practitioners, algorithm designers and customers, right from the start. We will then self-organize to achieve our aim of sharing our pooled experience and shaping it into practical, searchable and up-to-date advice on leveraging domain knowledge. [1] L. Davis, Handbook of genetic algorithms. Chapman & Hall, London, 1991.","title":"Welcome to How to tailor"},{"location":"#example-collection","text":"As a first step, we are aiming to collect examples. Please add your examples (see below).","title":"Example Collection"},{"location":"examples/example_1/","text":"Optimisation for a Fleet of Healthcare Vehicles Problem Description A healthcare provider in a region of Scotland (Argyll and Bute) wanted to reduce their vehicle fleet size while still being able to cater for all trips. They provided 4 months of historical data about where their existing fleet were based and the trips they conducted, including start and end times and geographic location. We were also given information about the vehicle types and which vehicles were allowed to do which trips. Why was tailoring needed? Not too much tailoring was needed but there were some particulars that had to be accounted for: Jobs (i.e. trips) have a type of vehicle which (historically) executed them, but if needed certain other types of vehicles can do the trip. For example, a small car originally did the trip, can be done by a van. Vehicles can be swapped between geographical bases if needed and if the swap does not mean that the vehicle home base cannot cover its own trips. It does not make sense to try and remove a type of vehicle from a base if there are none there or maybe if there are a small amount there. This led to a semi-guided mutation design. Baseline algorithm Upper level: stochastic local search; lower level: constructive heuristic. Motivations for choice: we wanted to keep it simple as possible and explainable for the user. No need to use fancy algorithms if a simple approach can obtain results. Tailoring process Adding in constraints (part of the operators); added additional vehicle/machine swap operation; semi-guided mutation. What was tailored Aspects of the algorithmic operators were tailored. This included the nature of the mutation operator and how it ensured that mutated solutions are feasible within the specific constraints of the problem. Main problem characteristics Choose most important ones: low-dimensional at upper level, high-dimensional at lower level; highly constrained (some soft and some hard); offline; there is an existing solution that works(current fleet); is a simplified version of what is eventually sought (optimising routes, carbon as well); low data sensitivity. References No response Author Sarah Thomson","title":"Optimisation for a Fleet of Healthcare Vehicles"},{"location":"examples/example_1/#optimisation-for-a-fleet-of-healthcare-vehicles","text":"","title":"Optimisation for a Fleet of Healthcare Vehicles"},{"location":"examples/example_1/#problem-description","text":"A healthcare provider in a region of Scotland (Argyll and Bute) wanted to reduce their vehicle fleet size while still being able to cater for all trips. They provided 4 months of historical data about where their existing fleet were based and the trips they conducted, including start and end times and geographic location. We were also given information about the vehicle types and which vehicles were allowed to do which trips.","title":"Problem Description"},{"location":"examples/example_1/#why-was-tailoring-needed","text":"Not too much tailoring was needed but there were some particulars that had to be accounted for: Jobs (i.e. trips) have a type of vehicle which (historically) executed them, but if needed certain other types of vehicles can do the trip. For example, a small car originally did the trip, can be done by a van. Vehicles can be swapped between geographical bases if needed and if the swap does not mean that the vehicle home base cannot cover its own trips. It does not make sense to try and remove a type of vehicle from a base if there are none there or maybe if there are a small amount there. This led to a semi-guided mutation design.","title":"Why was tailoring needed?"},{"location":"examples/example_1/#baseline-algorithm","text":"Upper level: stochastic local search; lower level: constructive heuristic. Motivations for choice: we wanted to keep it simple as possible and explainable for the user. No need to use fancy algorithms if a simple approach can obtain results.","title":"Baseline algorithm"},{"location":"examples/example_1/#tailoring-process","text":"Adding in constraints (part of the operators); added additional vehicle/machine swap operation; semi-guided mutation.","title":"Tailoring process"},{"location":"examples/example_1/#what-was-tailored","text":"Aspects of the algorithmic operators were tailored. This included the nature of the mutation operator and how it ensured that mutated solutions are feasible within the specific constraints of the problem.","title":"What was tailored"},{"location":"examples/example_1/#main-problem-characteristics","text":"Choose most important ones: low-dimensional at upper level, high-dimensional at lower level; highly constrained (some soft and some hard); offline; there is an existing solution that works(current fleet); is a simplified version of what is eventually sought (optimising routes, carbon as well); low data sensitivity.","title":"Main problem characteristics"},{"location":"examples/example_1/#references","text":"No response","title":"References"},{"location":"examples/example_1/#author","text":"Sarah Thomson","title":"Author"},{"location":"examples/example_2/","text":"Building spatial design Problem Description Optimise the spatial layout of a building to Minimise energy consumption for climate control, and Minimise the strain on the structure Why was tailoring needed? Many infeasible solutions exist. No algorithm existed/was found that could handle both multiple objectives, and a mixed-variable search space. Baseline algorithm SMS-EMOA. This performed best after initial tailoring steps needed to be able to run an algorithm at all, that were applied to both SMS-EMOA and NSGA-II. Tailoring process Design a superstructure problem representation to ensure the number of variables stays fixed (the existing representation would change the number of variables regularly). This made it possible to use standard EA frameworks . Modify algorithms designed for a single variable type to handle two variable types. ( Add existing standard mutation + crossover operators to handle variable types the algorithm cannot handle natively.) This made it possible to run an algorithm at all. Add equal penalty value for any solution that is infeasible . This led to only a very small number of feasible solutions, and not much optimisation yet. Penalty value based on the number of constraint violations (larger penalty for more violations). This led to more feasible solutions, and actually being able to optimise something. Design a problem-specific initialisation operator to ensure all initial solutions are feasible. This, combined with the next three steps, led to much better Pareto front approximations. Design a problem-specific mutation operator for the binary variables to ensure (standard operator was used for continuous variables, since there were no constraints on those) Remove the standard crossover operator , because it would cause many constraint violations (a new problem-specific crossover operator might be designed later, but proved to be very difficult to do) Add repair function was used to proportionally scale the continuous variables after mutation to match an equality constraint on them. Tune algorithm parameters to further improve performance. This led to further Pareto front approximation improvements. Add a local search step to try to improve solutions further by fixing the binary variables and optimising the continuous variables within that subspace.. This did not make much of a difference in the quality of the Pareto front approximation. Partial success : Although the developed approach worked in principle, it was quite limited in the size of the designs it could handle. For larger designs more modifications would be needed to ensure the problem-specific operators would not get stuck in a (fairly) local region of the search space. This was because the probability of finding a feasible mutation decreased when more modifications were made to the parent. (For small enough designs, the whole space would be local enough for things to work.) What was tailored No response Main problem characteristics Many hard constraints (simulator cannot evaluate the solution if these are violated) / Large part of the search space was infeasible. Checking if / how many constraints are violated is cheap. Mixed-variable search space (continuous + binary) Multiple objectives (Somewhat) expensive solution evaluations; with larger designs being more expensive. E.g., rough 1 second per evaluation for the smallest considered design, and roughly 40 seconds for the larger designs we considered. (Even the larger designs we considered are still relatively small for the considered problem.) Because of the above, we restricted ourselves to 2500 evaluations, but this was not a strict requirement. References No response Author Koen van der Blom","title":"Building spatial design"},{"location":"examples/example_2/#building-spatial-design","text":"","title":"Building spatial design"},{"location":"examples/example_2/#problem-description","text":"Optimise the spatial layout of a building to Minimise energy consumption for climate control, and Minimise the strain on the structure","title":"Problem Description"},{"location":"examples/example_2/#why-was-tailoring-needed","text":"Many infeasible solutions exist. No algorithm existed/was found that could handle both multiple objectives, and a mixed-variable search space.","title":"Why was tailoring needed?"},{"location":"examples/example_2/#baseline-algorithm","text":"SMS-EMOA. This performed best after initial tailoring steps needed to be able to run an algorithm at all, that were applied to both SMS-EMOA and NSGA-II.","title":"Baseline algorithm"},{"location":"examples/example_2/#tailoring-process","text":"Design a superstructure problem representation to ensure the number of variables stays fixed (the existing representation would change the number of variables regularly). This made it possible to use standard EA frameworks . Modify algorithms designed for a single variable type to handle two variable types. ( Add existing standard mutation + crossover operators to handle variable types the algorithm cannot handle natively.) This made it possible to run an algorithm at all. Add equal penalty value for any solution that is infeasible . This led to only a very small number of feasible solutions, and not much optimisation yet. Penalty value based on the number of constraint violations (larger penalty for more violations). This led to more feasible solutions, and actually being able to optimise something. Design a problem-specific initialisation operator to ensure all initial solutions are feasible. This, combined with the next three steps, led to much better Pareto front approximations. Design a problem-specific mutation operator for the binary variables to ensure (standard operator was used for continuous variables, since there were no constraints on those) Remove the standard crossover operator , because it would cause many constraint violations (a new problem-specific crossover operator might be designed later, but proved to be very difficult to do) Add repair function was used to proportionally scale the continuous variables after mutation to match an equality constraint on them. Tune algorithm parameters to further improve performance. This led to further Pareto front approximation improvements. Add a local search step to try to improve solutions further by fixing the binary variables and optimising the continuous variables within that subspace.. This did not make much of a difference in the quality of the Pareto front approximation. Partial success : Although the developed approach worked in principle, it was quite limited in the size of the designs it could handle. For larger designs more modifications would be needed to ensure the problem-specific operators would not get stuck in a (fairly) local region of the search space. This was because the probability of finding a feasible mutation decreased when more modifications were made to the parent. (For small enough designs, the whole space would be local enough for things to work.)","title":"Tailoring process"},{"location":"examples/example_2/#what-was-tailored","text":"No response","title":"What was tailored"},{"location":"examples/example_2/#main-problem-characteristics","text":"Many hard constraints (simulator cannot evaluate the solution if these are violated) / Large part of the search space was infeasible. Checking if / how many constraints are violated is cheap. Mixed-variable search space (continuous + binary) Multiple objectives (Somewhat) expensive solution evaluations; with larger designs being more expensive. E.g., rough 1 second per evaluation for the smallest considered design, and roughly 40 seconds for the larger designs we considered. (Even the larger designs we considered are still relatively small for the considered problem.) Because of the above, we restricted ourselves to 2500 evaluations, but this was not a strict requirement.","title":"Main problem characteristics"},{"location":"examples/example_2/#references","text":"No response","title":"References"},{"location":"examples/example_2/#author","text":"Koen van der Blom","title":"Author"},{"location":"examples/example_3/","text":"Combined Topology and Fibre-Orientation via Moving Morphable Components and Lamination Parameters Problem Description The idea is to optimize the topology or layout of a structure and the internal material layout. The topology was modelled via Moving Morphable Components (MMCs) wherein prescribed shapes are used to deform and move throughout a design 2D space. On the other hand, the fiber orientation was optimized by using the Lamination Parameter Formulation, which requires setting two parameters with global effects at some master nodes and the local fiber orientations are found by interpolation of these two parameters with respect to proximality to the master nodes. Why was tailoring needed? Normally, Topology Optimization requires more than 1000 design variables in most used formulations for this matter. Therefore, the Moving Morphable Components formulation was used to reduce dimensionality in a way that is tractable for black-box optimizers, but paying the cost of a much more reduced solution space. Simulation malfunctions required to be included as \u201cobjective constraints\u201d since some design combinations are numerically infeasible in the way the structure became kinematic and the underlying boundary conditions of the Finite Element solver weren\u2019t fulfilled. By evaluating the target for those sections of the search space, the algorithms get stuck since the target values are uninformative. For surrogate assisted optimization this is a big problem due to spurious correlations and discontinuities. Baseline algorithm No response Tailoring process Changed the target into a piecewise definition by evaluating first if the underlying structure complied with the boundary conditions. In other words, at least there was material connecting the structure from the support and there was material next to the load application. We adapted the function to handle constraints in an Augmented Lagrangian Fashion. We observed not so many works in the intersection of small feasible regions of search spaces and high-dimensional settings. However this may be counterproductive for Bayesian Optimization due to the discontinuity of the intersection of the feasible and unfeasible regions. What was tailored No response Main problem characteristics Continuous formulation of an inherent combinatorial high-dimensional space. Small feasible region or highly constrained Not all the search space can be evaluated and are meaningful. References No response Author Iv\u00e1n Olarte Rodr\u00edguez, Gokhan Serhat, Mariusz Bujny, Thomas Baeck, Elena Raponi","title":"Combined Topology and Fibre-Orientation via Moving Morphable Components and Lamination Parameters"},{"location":"examples/example_3/#combined-topology-and-fibre-orientation-via-moving-morphable-components-and-lamination-parameters","text":"","title":"Combined Topology and Fibre-Orientation via Moving Morphable Components and Lamination Parameters"},{"location":"examples/example_3/#problem-description","text":"The idea is to optimize the topology or layout of a structure and the internal material layout. The topology was modelled via Moving Morphable Components (MMCs) wherein prescribed shapes are used to deform and move throughout a design 2D space. On the other hand, the fiber orientation was optimized by using the Lamination Parameter Formulation, which requires setting two parameters with global effects at some master nodes and the local fiber orientations are found by interpolation of these two parameters with respect to proximality to the master nodes.","title":"Problem Description"},{"location":"examples/example_3/#why-was-tailoring-needed","text":"Normally, Topology Optimization requires more than 1000 design variables in most used formulations for this matter. Therefore, the Moving Morphable Components formulation was used to reduce dimensionality in a way that is tractable for black-box optimizers, but paying the cost of a much more reduced solution space. Simulation malfunctions required to be included as \u201cobjective constraints\u201d since some design combinations are numerically infeasible in the way the structure became kinematic and the underlying boundary conditions of the Finite Element solver weren\u2019t fulfilled. By evaluating the target for those sections of the search space, the algorithms get stuck since the target values are uninformative. For surrogate assisted optimization this is a big problem due to spurious correlations and discontinuities.","title":"Why was tailoring needed?"},{"location":"examples/example_3/#baseline-algorithm","text":"No response","title":"Baseline algorithm"},{"location":"examples/example_3/#tailoring-process","text":"Changed the target into a piecewise definition by evaluating first if the underlying structure complied with the boundary conditions. In other words, at least there was material connecting the structure from the support and there was material next to the load application. We adapted the function to handle constraints in an Augmented Lagrangian Fashion. We observed not so many works in the intersection of small feasible regions of search spaces and high-dimensional settings. However this may be counterproductive for Bayesian Optimization due to the discontinuity of the intersection of the feasible and unfeasible regions.","title":"Tailoring process"},{"location":"examples/example_3/#what-was-tailored","text":"No response","title":"What was tailored"},{"location":"examples/example_3/#main-problem-characteristics","text":"Continuous formulation of an inherent combinatorial high-dimensional space. Small feasible region or highly constrained Not all the search space can be evaluated and are meaningful.","title":"Main problem characteristics"},{"location":"examples/example_3/#references","text":"No response","title":"References"},{"location":"examples/example_3/#author","text":"Iv\u00e1n Olarte Rodr\u00edguez, Gokhan Serhat, Mariusz Bujny, Thomas Baeck, Elena Raponi","title":"Author"},{"location":"examples/example_4/","text":"Antenna control Problem Description tune a phased array antenna to steer it towards a target satellite, for telecommunication Why was tailoring needed? existing algorithms were either too inefficient, or required too many function evaluations Baseline algorithm surrogate-based optimization. To reduce the required number of function evaluations. Tailoring process The goal was to develop an algorithm that is efficient in the number of required function evaluations, but also in the computation time needed to propose a new candidate solution. The chosen framework was the same as in surrogate-based optimization: start with a candidate solution, evaluate the solution, update a surrogate model on the data gathered so far, then use the surrogate model to guide the search towards the optimal solution. The idea was that this framework would need less function evaluations than other black-box optimization algorithms such as evolutionary algorithms, though this was not tested. The chosen surrogate model was a Random Fourier Expansion, as it is efficient to train and to update, does not slow down over time like Gaussian processes, and has theoretical guarantees available. The chosen acquisition function was a type of epsilon-greedy exploration with local perturbations. This allowed for a fast convergence towards a local optimum, while not getting stuck at a local optimum. It avoided the need of spending computation time on calculating covariances or optimizing surrogate model hyperparameters. Hyperparameters of the approach were chosen based on expertise and initial experiments. For example, the number of basis functions was a hyperparameter that needed to be chosen such that it balances computational efficiency and performance. What was not successful: Nelder-Mead simplex method Powell\u2019s method using a quadratic surrogate model same as proposed approach but pure greedy (epsilon=0) \u2192 got stuck in local optima Bayesian optimization with Gaussian processes a more white-box approach that tried to linearize the problem Result good enough solution found within 2 minutes, with 3000 function evaluations solution 3000 times faster than Bayesian optimization with Gaussian processes solution twice as good as a predict-then-optimize approach with the same surrogate model What was tailored an algorithm was developed from scratch, with the same framework as other surrogate-based optimization algorithms. Main problem characteristics 24 continuous variables with lower and upper bounds defined and no other constraints relatively cheap but noisy objective black-box, single-objective, single-fidelity, no parallelization Time constraint of 2 minutes (8 years ago, maybe now it would be 1 second or something) Function evaluation constraint of 3000 tested on a simulator, with the goal of eventually being applied on a physical system References optimal beam-forming network tuning more information on the application (not the algorithm) Contact information (optional) No response Author Laurens Bliek No response","title":"Antenna control"},{"location":"examples/example_4/#antenna-control","text":"","title":"Antenna control"},{"location":"examples/example_4/#problem-description","text":"tune a phased array antenna to steer it towards a target satellite, for telecommunication","title":"Problem Description"},{"location":"examples/example_4/#why-was-tailoring-needed","text":"existing algorithms were either too inefficient, or required too many function evaluations","title":"Why was tailoring needed?"},{"location":"examples/example_4/#baseline-algorithm","text":"surrogate-based optimization. To reduce the required number of function evaluations.","title":"Baseline algorithm"},{"location":"examples/example_4/#tailoring-process","text":"The goal was to develop an algorithm that is efficient in the number of required function evaluations, but also in the computation time needed to propose a new candidate solution. The chosen framework was the same as in surrogate-based optimization: start with a candidate solution, evaluate the solution, update a surrogate model on the data gathered so far, then use the surrogate model to guide the search towards the optimal solution. The idea was that this framework would need less function evaluations than other black-box optimization algorithms such as evolutionary algorithms, though this was not tested. The chosen surrogate model was a Random Fourier Expansion, as it is efficient to train and to update, does not slow down over time like Gaussian processes, and has theoretical guarantees available. The chosen acquisition function was a type of epsilon-greedy exploration with local perturbations. This allowed for a fast convergence towards a local optimum, while not getting stuck at a local optimum. It avoided the need of spending computation time on calculating covariances or optimizing surrogate model hyperparameters. Hyperparameters of the approach were chosen based on expertise and initial experiments. For example, the number of basis functions was a hyperparameter that needed to be chosen such that it balances computational efficiency and performance. What was not successful: Nelder-Mead simplex method Powell\u2019s method using a quadratic surrogate model same as proposed approach but pure greedy (epsilon=0) \u2192 got stuck in local optima Bayesian optimization with Gaussian processes a more white-box approach that tried to linearize the problem Result good enough solution found within 2 minutes, with 3000 function evaluations solution 3000 times faster than Bayesian optimization with Gaussian processes solution twice as good as a predict-then-optimize approach with the same surrogate model","title":"Tailoring process"},{"location":"examples/example_4/#what-was-tailored","text":"an algorithm was developed from scratch, with the same framework as other surrogate-based optimization algorithms.","title":"What was tailored"},{"location":"examples/example_4/#main-problem-characteristics","text":"24 continuous variables with lower and upper bounds defined and no other constraints relatively cheap but noisy objective black-box, single-objective, single-fidelity, no parallelization Time constraint of 2 minutes (8 years ago, maybe now it would be 1 second or something) Function evaluation constraint of 3000 tested on a simulator, with the goal of eventually being applied on a physical system","title":"Main problem characteristics"},{"location":"examples/example_4/#references","text":"optimal beam-forming network tuning more information on the application (not the algorithm)","title":"References"},{"location":"examples/example_4/#contact-information-optional","text":"No response","title":"Contact information (optional)"},{"location":"examples/example_4/#author","text":"Laurens Bliek No response","title":"Author"},{"location":"examples/example_template/","text":"${TITLE} Problem Description ${PROBLEM} Why was tailoring needed? ${WHY} Baseline algorithm ${BASELINE} Tailoring process ${PROCESS} What was tailored ${WHAT} Main problem characteristics ${CHAR} References ${REFERENCES} Author ${AUTHOR} ${CONTACT}","title":"${TITLE}"},{"location":"examples/example_template/#title","text":"","title":"${TITLE}"},{"location":"examples/example_template/#problem-description","text":"${PROBLEM}","title":"Problem Description"},{"location":"examples/example_template/#why-was-tailoring-needed","text":"${WHY}","title":"Why was tailoring needed?"},{"location":"examples/example_template/#baseline-algorithm","text":"${BASELINE}","title":"Baseline algorithm"},{"location":"examples/example_template/#tailoring-process","text":"${PROCESS}","title":"Tailoring process"},{"location":"examples/example_template/#what-was-tailored","text":"${WHAT}","title":"What was tailored"},{"location":"examples/example_template/#main-problem-characteristics","text":"${CHAR}","title":"Main problem characteristics"},{"location":"examples/example_template/#references","text":"${REFERENCES}","title":"References"},{"location":"examples/example_template/#author","text":"${AUTHOR} ${CONTACT}","title":"Author"}]}